import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from datetime import datetime
import os

import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import pickle
def train_mlp_from_csv(csv_path="depreciation_records.csv", output_path="depreciation_mlp.pth"):
    """
    å¾ CSV è³‡æ–™è¨“ç·´æˆ–å¾®èª¿æŠ˜èˆŠåˆ†æç”¨çš„ MLP æ¨¡å‹ï¼Œä¸¦å„²å­˜ç‚º .pth æª”æ¡ˆã€‚
    è‹¥æ¨¡å‹å·²å­˜åœ¨ï¼Œå‰‡è¼‰å…¥ä¸¦ç¹¼çºŒè¨“ç·´ï¼›å¦å‰‡æ–°å»ºæ¨¡å‹ã€‚
    """

    # ğŸ“¥ è®€å–è³‡æ–™
    df = pd.read_csv(csv_path)
    X = torch.tensor(df[["defect_index", "avg_depth", "max_depth", "total_area"]].values, dtype=torch.float32)
    y = torch.tensor(df["grade"].map({"A - normal": 0, "B - Under_observation": 1, "C - Recommended_repair": 2}).values, dtype=torch.long)

    # ğŸ“¦ å»ºç«‹è³‡æ–™é›†èˆ‡ DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)

    # ğŸ§  æ¨¡å‹å»ºç«‹æˆ–è¼‰å…¥
    model = DepreciationMLP()
    if os.path.exists(output_path):
        print(f"ğŸ“‚ åµæ¸¬åˆ°å·²å­˜åœ¨æ¨¡å‹ {output_path}ï¼Œå°‡è¼‰å…¥ä¸¦ç¹¼çºŒè¨“ç·´")
        model.load_state_dict(torch.load(output_path,weights_only=True))  # è¼‰å…¥æ¬Šé‡

    # âš™ï¸ è¨“ç·´å…ƒä»¶è¨­å®š
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    # ğŸ” æ¨¡å‹è¨“ç·´è¿´åœˆ
    for epoch in range(50):
        for batch_x, batch_y in loader:
            optimizer.zero_grad()
            logits = model(batch_x)
            loss = criterion(logits, batch_y)
            loss.backward()
            optimizer.step()

    # ğŸ’¾ å„²å­˜æ¨¡å‹
    # torch.save(model, output_path)
    torch.save(model.state_dict(), output_path)  # åªå„²å­˜æ¬Šé‡
    print(f"âœ… æ¨¡å‹å·²è¨“ç·´ä¸¦å„²å­˜è‡³ {output_path}")

def compute_depreciation_metrics(defects):
    """
    ğŸ“Œ ç”¨é€”ï¼šæ ¹æ“šç¼ºé™·æ¸…å–®è¨ˆç®—æŠ˜èˆŠåˆ†æçš„æ ¸å¿ƒæŒ‡æ¨™(defect_index)ã€‚
    æ¯å€‹ç¼ºé™·çš„å½±éŸ¿åŠ›ä»¥ã€Œé¢ç© Ã— æ·±åº¦ã€è¡¨ç¤ºï¼Œä¸¦çµ±è¨ˆæ•´é«”ç¼ºé™·æ•¸é‡ã€å¹³å‡æ·±åº¦ã€æœ€å¤§æ·±åº¦èˆ‡ç¸½é¢ç©ã€‚
    ğŸ”¢ å›å‚³å…§å®¹ï¼š
    - defect_indexï¼šæŠ˜èˆŠæŒ‡æ•¸(é¢ç© Ã— æ·±åº¦ çš„åŠ ç¸½)
    - defect_countï¼šç¼ºé™·æ•¸é‡
    - avg_depthï¼šå¹³å‡æ·±åº¦
    - max_depthï¼šæœ€å¤§æ·±åº¦
    - total_areaï¼šæ‰€æœ‰ç¼ºé™·çš„ç¸½é¢ç©
    """
    if not defects:
        return {
            "defect_index": 0,
            "defect_count": 0,
            "avg_depth": 0,
            "max_depth": 0,
            "total_area": 0
        }

    defect_index = sum([d['area'] * d['depth'] for d in defects])
    avg_depth = np.mean([d['depth'] for d in defects])
    max_depth = np.max([d['depth'] for d in defects])
    total_area = sum([d['area'] for d in defects])
    return {
        "defect_index": defect_index,
        "defect_count": len(defects),
        "avg_depth": avg_depth,
        "max_depth": max_depth,
        "total_area": total_area
    }

def classify_depreciation(defect_index):
    """
    ğŸ“Œ ç”¨é€”ï¼šæ ¹æ“šæŠ˜èˆŠæŒ‡æ•¸(defect_index)é€²è¡Œåˆ†ç´šåˆ¤æ–·ï¼Œå”åŠ©ä½¿ç”¨è€…å¿«é€Ÿäº†è§£ç‰©ä»¶ç›®å‰çš„å¥åº·ç‹€æ…‹ã€‚
    ğŸ“Š åˆ†ç´šé‚è¼¯ï¼š
    - Aï¼šnormal(defect_index < 50)
    - Bï¼šUnder_observation(50 â‰¤ defect_index < 150)
    - Cï¼šRecommended_repair(defect_index â‰¥ 150)
    ğŸ” å›å‚³ï¼š
    å°æ‡‰çš„æŠ˜èˆŠç­‰ç´šå­—ä¸²(å«å»ºè­°)
    """
    """åŸºæ–¼ä½ æ•¸æ“šçš„25%å’Œ75%ç™¾åˆ†ä½æ•¸ï¼Œèƒ½ç¢ºä¿ï¼š
        ç´„25%çš„æ•¸æ“šç‚ºAç´š(normal)
        ç´„50%çš„æ•¸æ“šç‚ºBç´š(Under_observation)
        ç´„25%çš„æ•¸æ“šç‚ºCç´š(Recommended_repair)
        å…¶ä»–è€ƒæ…®çš„æ–¹æ¡ˆï¼š
        æ¨™æº–å·®æ³•ï¼šA<2971, B<6225, Câ‰¥6225
        ä¿å®ˆæ³•ï¼šA<4631, B<6741, Câ‰¥6741 """
    """"é›–ç„¶è¦å‰‡å¼åˆ†é¡åœ¨åˆæœŸå¾ˆå¯¦ç”¨ï¼Œä½†éš¨è‘—æ•¸æ“šç´¯ç©ï¼Œ
    æ©Ÿå™¨å­¸ç¿’æ–¹æ³•é€šå¸¸èƒ½æä¾›æ›´å¥½çš„åˆ†é¡æº–ç¢ºæ€§å’Œé©æ‡‰æ€§ã€‚
    MyDefectDetection ç³»çµ±çš„æ··åˆæ¶æ§‹è¨­è¨ˆé«”ç¾äº†é€™ç¨®æ¼¸é€²å¼æ”¹é€²çš„æœ€ä½³å¯¦è¸ã€‚"""

    if defect_index < 3876:
        return "A - normal"
    elif defect_index < 5554:
        return "B - Under_observation"
    else:
        return "C - Recommended_repair"

def generate_depreciation_record(defects,mlp_model=None):
    """
    ğŸ“Œ ç”¨é€”ï¼šæ•´åˆæŠ˜èˆŠåˆ†ææµç¨‹ï¼Œç”Ÿæˆä¸€ç­†å®Œæ•´çš„ç´€éŒ„ã€‚
    åŒ…å«åˆ†ææ™‚é–“ã€æŠ˜èˆŠç­‰ç´šã€å„é …æŒ‡æ¨™èˆ‡åŸå§‹ç¼ºé™·æ¸…å–®ï¼Œæ–¹ä¾¿å„²å­˜ã€è¿½è¹¤èˆ‡å¯è¦–åŒ–ã€‚
    ğŸ§© çµ„æˆï¼š
    - timestampï¼šåˆ†ææ™‚é–“(æ ¼å¼ï¼šYYYY-MM-DD HH:MM)
    - gradeï¼šæŠ˜èˆŠç­‰ç´š(ç”± classify_depreciation åˆ¤æ–·)
    - defect_index / defect_count / avg_depth / max_depth / total_areaï¼šç”± compute_depreciation_metrics è¨ˆç®—
    - defectsï¼šåŸå§‹ç¼ºé™·æ¸…å–®(å«é¢ç©ã€æ·±åº¦ã€ä½ç½®ç­‰)
    ğŸ” å›å‚³ï¼š
    ä¸€å€‹ dict çµæ§‹çš„æŠ˜èˆŠåˆ†æç´€éŒ„
    """
    metrics = compute_depreciation_metrics(defects)
    if mlp_model: #ä½¿ç”¨ MLP æ¨¡å‹æ ¹æ“šç¼ºé™·æŒ‡æ¨™é æ¸¬æŠ˜èˆŠç­‰ç´šã€‚
        grade, confidence = classify_depreciation_mlp(metrics, mlp_model)
    else:       #ä½¿ç”¨ç°¡å–®çš„é–¾å€¼åˆ†é¡
        grade = classify_depreciation(metrics["defect_index"])
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    return {
        "timestamp": timestamp,
        "grade": grade,
        "confidence": confidence if mlp_model else "N/A",
        **metrics,
        "defects": defects
    }

import torch.nn as nn
class DepreciationMLP(nn.Module):
    """ æŠ˜èˆŠåˆ†æç”¨çš„å¤šå±¤æ„ŸçŸ¥å™¨(MLP)æ¨¡å‹
    Linear 1 -
    æ¥æ”¶æŠ˜èˆŠæŒ‡æ¨™: defect_indexã€avg_depthã€max_depthã€total_area å››å€‹è¼¸å…¥ç‰¹å¾µï¼Œ
    é æ¸¬æŠ˜èˆŠç­‰ç´š(Aã€Bã€C ä¸‰é¡)ã€‚
    è¼¸å…¥ç¶­åº¦:4(defect_indexã€avg_depthã€max_depthã€total_area)
    éš±è—å±¤ç¶­åº¦:16
    æ¿€æ´»å‡½æ•¸:ReLU éç·šæ€§è½‰æ›
    Linear 2 -
    è¼¸å‡ºç¶­åº¦:3(å°æ‡‰ä¸‰å€‹æŠ˜èˆŠç­‰ç´šé¡åˆ¥ A/B/C)
    ğŸ” å‰å‘å‚³æ’­æµç¨‹:
    è¼¸å…¥ â†’ ç·šæ€§å±¤1 â†’ ReLU â†’ ç·šæ€§å±¤2 â†’ è¼¸å‡º logits
    """
    def __init__(self, input_dim=4, hidden_dim=16, output_dim=3):
        super().__init__()# åˆå§‹åŒ– nn.Module çˆ¶é¡åˆ¥
        # ğŸ§  å®šç¾© MLP æ¶æ§‹ï¼šè¼¸å…¥å±¤ â†’ éš±è—å±¤ â†’ è¼¸å‡ºå±¤
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),# ç·šæ€§å±¤ï¼šè¼¸å…¥ç¶­åº¦ â†’ éš±è—å±¤ç¶­åº¦
            nn.ReLU(),# å•Ÿç”¨å‡½æ•¸ï¼šReLU éç·šæ€§è½‰æ›
            nn.Linear(hidden_dim, output_dim)# ç·šæ€§å±¤ï¼šéš±è—å±¤ â†’ è¼¸å‡ºé¡åˆ¥æ•¸
        )
    def forward(self, x):
        return self.model(x)# å‰å‘å‚³æ’­ï¼šå°‡è¼¸å…¥ x å‚³å…¥æ¨¡å‹ä¸¦å›å‚³è¼¸å‡º

def classify_depreciation_mlp(metrics, mlp_model):
    """
    ä½¿ç”¨ MLP æ¨¡å‹æ ¹æ“šç¼ºé™·æŒ‡æ¨™é æ¸¬æŠ˜èˆŠç­‰ç´šï¼Œä¸¦å›å‚³ä¿¡å¿ƒåˆ†æ•¸ã€‚
    ğŸ”¢ è¼¸å…¥ï¼šmetrics dict(åŒ…å« defect_indexã€avg_depthã€max_depthã€total_area)
    ğŸ” å›å‚³ï¼štuple â†’ (æŠ˜èˆŠç­‰ç´šå­—ä¸², ä¿¡å¿ƒåˆ†æ•¸)
    """
    # ğŸ§® å°‡æŒ‡æ¨™è½‰ç‚ºå¼µé‡ä¸¦åŠ ä¸Š batch ç¶­åº¦
    input_tensor = torch.tensor([
        metrics["defect_index"],
        metrics["avg_depth"],
        metrics["max_depth"],
        metrics["total_area"]
    ], dtype=torch.float32).unsqueeze(0)

    # ğŸš« æ¨è«–æ¨¡å¼(åœç”¨æ¢¯åº¦)
    with torch.no_grad():
        logits = mlp_model(input_tensor)  # å‰å‘å‚³æ’­,logits æ˜¯ MLP æ¨¡å‹çš„åŸå§‹è¼¸å‡º(æœªç¶“æ¨™æº–åŒ–)ï¼Œé€šå¸¸æ˜¯æ¯å€‹é¡åˆ¥çš„åˆ†æ•¸
        probs = torch.softmax(logits, dim=1)  # è¨ˆç®— softmax æ©Ÿç‡åˆ†å¸ƒï¼Œsoftmax å°‡ logits è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒï¼Œä½¿æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡åŠ ç¸½ç‚º 1ã€‚
        #EX:logits = [1.2, 3.5, 0.8] (æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡åŠ ç¸½ç‚º1æ™‚)-> probs = [0.12, 0.82, 0.06]
        pred = torch.argmax(probs, dim=1).item()  # å–å¾—é æ¸¬é¡åˆ¥ç´¢å¼•ï¼Œpred æ˜¯æœ€å¤§æ©Ÿç‡çš„é¡åˆ¥ç´¢å¼•(å³æ¨¡å‹é æ¸¬çš„åˆ†é¡)ã€‚
        confidence = probs[0, pred].item()  # å–å¾—è©²é¡åˆ¥çš„ä¿¡å¿ƒåˆ†æ•¸ï¼Œæ˜¯è©²é¡åˆ¥çš„æ©Ÿç‡å€¼ï¼Œä»£è¡¨æ¨¡å‹å°é€™å€‹é æ¸¬çš„ä¿¡å¿ƒã€‚

    # ğŸ“¤ å›å‚³ç­‰ç´šèˆ‡ä¿¡å¿ƒåˆ†æ•¸
    label = ["A - normal", "B - Under_observation", "C - Recommended_repair"]
    # ä¿¡å¿ƒåˆ†æ•¸(confidence)çš„ç”¨é€”ï¼š
    # å ±è¡¨å‘ˆç¾ï¼šè®“ä½¿ç”¨è€…çŸ¥é“é æ¸¬æ˜¯å¦å¯é 
    # è­¦ç¤ºæ©Ÿåˆ¶ï¼šè‹¥ä¿¡å¿ƒä½æ–¼æŸé–€æª»(ä¾‹å¦‚ 0.6)ï¼Œå¯æ¨™ç¤ºç‚ºã€Œä¸ç¢ºå®šã€
    # æ¨¡å‹è©•ä¼°ï¼šå¯ç”¨æ–¼ ROC æ›²ç·šã€Precision-Recall åˆ†æ
    # æ±ºç­–æ”¯æ´ï¼šé«˜ä¿¡å¿ƒå¯è‡ªå‹•é€šéï¼Œä½ä¿¡å¿ƒå¯è½‰äººå·¥è¤‡æ ¸
    return label[pred], confidence

class EnhancedDepreciationMLP(nn.Module):
    """æ”¹è‰¯ç‰ˆæŠ˜èˆŠåˆ†æ MLP æ¨¡å‹
    - æ›´æ·±çš„ç¶²è·¯æ¶æ§‹ï¼ˆ3å±¤éš±è—å±¤ï¼‰
    - Dropout é˜²éæ“¬åˆ
    - BatchNorm ç©©å®šè¨“ç·´
    - æ”¯æ´æ›´å¤šç‰¹å¾µè¼¸å…¥
    """
    def __init__(self, input_dim=8, hidden_dims=[64, 32, 16], output_dim=3, dropout_rate=0.3):
        super().__init__()

        layers = []
        prev_dim = input_dim

        # å»ºç«‹å¤šå±¤éš±è—å±¤
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(0.1),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim

        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_dim, output_dim))

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

def compute_enhanced_depreciation_metrics(defects, image_shape=(256, 256), use_basic_features_only=True):
    """è¨ˆç®—å¢å¼·ç‰ˆæŠ˜èˆŠæŒ‡æ¨™ï¼Œå¯é¸æ“‡åªä½¿ç”¨åŸºç¤ç‰¹å¾µ"""
    # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºé™·è³‡æ–™ï¼Œè‹¥ç„¡å‰‡å›å‚³é›¶å€¼æŒ‡æ¨™
    if not defects:
        # å»ºç«‹åŸºç¤æŒ‡æ¨™çš„é›¶å€¼å­—å…¸
        basic_metrics = {
            "defect_index": 0, "defect_count": 0, "avg_depth": 0, "max_depth": 0, "total_area": 0
        }
        # æ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦åªå›å‚³åŸºç¤ç‰¹å¾µ
        if use_basic_features_only:
            return basic_metrics
        else:
            # å›å‚³åŒ…å«é¡å¤–ç‰¹å¾µçš„å®Œæ•´é›¶å€¼å­—å…¸
            return {**basic_metrics, "depth_std": 0, "area_ratio": 0, "defect_density": 0}

    # åŸºç¤æŒ‡æ¨™è¨ˆç®—ä¿æŒä¸è®Š - è¨ˆç®—æ ¸å¿ƒæŠ˜èˆŠæŒ‡æ¨™
    defect_index = sum([d['area'] * d['depth'] for d in defects])  # è¨ˆç®—æŠ˜èˆŠæŒ‡æ•¸ï¼ˆé¢ç©Ã—æ·±åº¦çš„ç¸½å’Œï¼‰
    depths = [d['depth'] for d in defects]  # æå–æ‰€æœ‰ç¼ºé™·çš„æ·±åº¦å€¼
    areas = [d['area'] for d in defects]    # æå–æ‰€æœ‰ç¼ºé™·çš„é¢ç©å€¼

    avg_depth = np.mean(depths)   # è¨ˆç®—å¹³å‡æ·±åº¦
    max_depth = np.max(depths)    # æ‰¾å‡ºæœ€å¤§æ·±åº¦
    total_area = sum(areas)       # è¨ˆç®—ç¸½é¢ç©

    # çµ„å»ºåŸºç¤æŒ‡æ¨™å­—å…¸
    basic_metrics = {
        "defect_index": defect_index,    # æŠ˜èˆŠæŒ‡æ•¸
        "defect_count": len(defects),    # ç¼ºé™·æ•¸é‡
        "avg_depth": avg_depth,          # å¹³å‡æ·±åº¦
        "max_depth": max_depth,          # æœ€å¤§æ·±åº¦
        "total_area": total_area         # ç¸½é¢ç©
    }

    # å¦‚æœåªéœ€è¦åŸºç¤ç‰¹å¾µï¼Œç›´æ¥å›å‚³
    if use_basic_features_only:
        return basic_metrics

    # é¡å¤–ç‰¹å¾µ - è¨ˆç®—é€²éšçµ±è¨ˆç‰¹å¾µ
    depth_std = np.std(depths) if len(depths) > 1 else 0  # è¨ˆç®—æ·±åº¦æ¨™æº–å·®ï¼ˆéœ€å¤šæ–¼1å€‹ç¼ºé™·ï¼‰
    total_image_area = image_shape[0] * image_shape[1]    # è¨ˆç®—å½±åƒç¸½é¢ç©
    area_ratio = total_area / total_image_area            # è¨ˆç®—é¢ç©æ¯”ä¾‹ï¼ˆç¼ºé™·é¢ç©/å½±åƒé¢ç©ï¼‰
    defect_density = len(defects) / total_image_area * 10000  # è¨ˆç®—ç¼ºé™·å¯†åº¦ï¼ˆæ¯è¬åƒç´ çš„ç¼ºé™·æ•¸ï¼‰

    # å›å‚³åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„å®Œæ•´å­—å…¸
    return {
        **basic_metrics,           # å±•é–‹åŸºç¤æŒ‡æ¨™
        "depth_std": depth_std,    # æ·±åº¦æ¨™æº–å·®
        "area_ratio": area_ratio,  # é¢ç©æ¯”ä¾‹
        "defect_density": defect_density  # ç¼ºé™·å¯†åº¦
    }

def train_enhanced_mlp_from_csv(csv_path="depreciation_records.csv",
                               output_path="enhanced_depreciation_mlp.pth",
                               scaler_path="feature_scaler.pkl",
                               epochs=100, patience=10):
    """æ”¹è‰¯ç‰ˆ MLP è¨“ç·´å‡½æ•¸
    - æ•¸æ“šæ¨™æº–åŒ–
    - è¨“ç·´/é©—è­‰åˆ†å‰²
    - æ—©åœæ©Ÿåˆ¶
    - å­¸ç¿’ç‡èª¿åº¦
    - é¡åˆ¥æ¬Šé‡å¹³è¡¡
    """

    # è®€å–æ•¸æ“š - å¾ CSV æª”æ¡ˆè¼‰å…¥æŠ˜èˆŠåˆ†æè¨˜éŒ„
    df = pd.read_csv(csv_path)

    # ç‰¹å¾µé¸æ“‡ï¼ˆæ”¯æ´æ›´å¤šç‰¹å¾µï¼‰- å…ˆè¨­å®šåŸºç¤ç‰¹å¾µé›†
    feature_cols = ["defect_index", "avg_depth", "max_depth", "total_area"]
    # æª¢æŸ¥æ˜¯å¦æœ‰é¡å¤–ç‰¹å¾µæ¬„ä½ï¼Œè‹¥æœ‰å‰‡æ“´å±•ç‰¹å¾µé›†
    if "depth_std" in df.columns:
        feature_cols.extend(["depth_std", "area_ratio", "defect_density"])

    # æå–ç‰¹å¾µçŸ©é™£å’Œæ¨™ç±¤å‘é‡
    X = df[feature_cols].values  # ç‰¹å¾µæ•¸æ“š
    # å°‡ç­‰ç´šå­—ä¸²æ˜ å°„ç‚ºæ•¸å€¼æ¨™ç±¤ (A=0, B=1, C=2)
    y = df["grade"].map({"A - normal": 0, "B - Under_observation": 1, "C - Recommended_repair": 2}).values

    # æ•¸æ“šæ¨™æº–åŒ– - å»ºç«‹æ¨™æº–åŒ–å™¨ä¸¦å°ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)  # è¨ˆç®—å‡å€¼å’Œæ¨™æº–å·®ä¸¦æ¨™æº–åŒ–

    # å„²å­˜æ¨™æº–åŒ–å™¨ - ä¿å­˜ä»¥ä¾›å¾ŒçºŒæ¨è«–ä½¿ç”¨
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)

    # è¨“ç·´/é©—è­‰åˆ†å‰² - åˆ†å±¤æŠ½æ¨£ç¢ºä¿å„é¡åˆ¥æ¯”ä¾‹ä¸€è‡´
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )

    # è½‰æ›ç‚ºå¼µé‡ - å°‡ NumPy é™£åˆ—è½‰æ›ç‚º PyTorch å¼µé‡
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # è¨“ç·´ç‰¹å¾µ
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)     # è¨“ç·´æ¨™ç±¤
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)      # é©—è­‰ç‰¹å¾µ
    y_val_tensor = torch.tensor(y_val, dtype=torch.long)         # é©—è­‰æ¨™ç±¤

    # è¨ˆç®—é¡åˆ¥æ¬Šé‡ - è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

    # å»ºç«‹æ¨¡å‹ - æ ¹æ“šç‰¹å¾µæ•¸é‡è¨­å®šè¼¸å…¥ç¶­åº¦
    model = EnhancedDepreciationMLP(input_dim=len(feature_cols))

    # è¼‰å…¥å·²å­˜åœ¨çš„æ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰- æ”¯æ´å¢é‡è¨“ç·´
    if os.path.exists(output_path):
        print(f"ğŸ“‚ è¼‰å…¥å·²å­˜åœ¨æ¨¡å‹ {output_path}")
        model.load_state_dict(torch.load(output_path, weights_only=True))

    # è¨“ç·´è¨­å®š - é…ç½®æå¤±å‡½æ•¸ã€å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)  # åŠ æ¬Šäº¤å‰ç†µæå¤±
    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)  # AdamW å„ªåŒ–å™¨
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)  # å­¸ç¿’ç‡èª¿åº¦

    # æ—©åœè¨­å®š - é˜²æ­¢éæ“¬åˆ
    best_val_loss = float('inf')  # è¨˜éŒ„æœ€ä½³é©—è­‰æå¤±
    patience_counter = 0          # è€å¿ƒè¨ˆæ•¸å™¨
    best_model_state = None       # æœ€ä½³æ¨¡å‹ç‹€æ…‹

    # è¨“ç·´è¿´åœˆ - å»ºç«‹æ•¸æ“šè¼‰å…¥å™¨ä¸¦é–‹å§‹è¨“ç·´
    train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)

    model.train()  # è¨­å®šç‚ºè¨“ç·´æ¨¡å¼
    for epoch in range(epochs):
        # è¨“ç·´éšæ®µ - æ‰¹æ¬¡è¨“ç·´
        train_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()    # æ¸…é›¶æ¢¯åº¦
            logits = model(batch_x)  # å‰å‘å‚³æ’­
            loss = criterion(logits, batch_y)  # è¨ˆç®—æå¤±
            loss.backward()          # åå‘å‚³æ’­
            optimizer.step()         # æ›´æ–°åƒæ•¸
            train_loss += loss.item()  # ç´¯ç©æå¤±

        # é©—è­‰éšæ®µ - è©•ä¼°æ¨¡å‹æ€§èƒ½
        model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
        with torch.no_grad():  # åœç”¨æ¢¯åº¦è¨ˆç®—
            val_logits = model(X_val_tensor)  # é©—è­‰é›†å‰å‘å‚³æ’­
            val_loss = criterion(val_logits, y_val_tensor).item()  # è¨ˆç®—é©—è­‰æå¤±
            val_acc = (torch.argmax(val_logits, dim=1) == y_val_tensor).float().mean().item()  # è¨ˆç®—æº–ç¢ºç‡

        # å­¸ç¿’ç‡èª¿åº¦ - æ ¹æ“šé©—è­‰æå¤±èª¿æ•´å­¸ç¿’ç‡
        scheduler.step(val_loss)

        # æ—©åœæª¢æŸ¥ - ç›£æ§é©—è­‰æå¤±æ”¹å–„æƒ…æ³
        if val_loss < best_val_loss:
            best_val_loss = val_loss  # æ›´æ–°æœ€ä½³æå¤±
            patience_counter = 0      # é‡ç½®è€å¿ƒè¨ˆæ•¸å™¨
            best_model_state = model.state_dict().copy()  # ä¿å­˜æœ€ä½³æ¨¡å‹ç‹€æ…‹
        else:
            patience_counter += 1     # å¢åŠ è€å¿ƒè¨ˆæ•¸å™¨

        # æ¯ 10 å€‹ epoch è¼¸å‡ºè¨“ç·´é€²åº¦
        if epoch % 10 == 0:
            print(f"Epoch {epoch}: Train Loss={train_loss/len(train_loader):.4f}, "
                  f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")

        # æ—©åœæ¢ä»¶æª¢æŸ¥ - è‹¥é©—è­‰æå¤±é•·æ™‚é–“æœªæ”¹å–„å‰‡åœæ­¢è¨“ç·´
        if patience_counter >= patience:
            print(f"æ—©åœæ–¼ epoch {epoch}")
            break

        model.train()  # å›åˆ°è¨“ç·´æ¨¡å¼

    # è¼‰å…¥æœ€ä½³æ¨¡å‹ - æ¢å¾©é©—è­‰æå¤±æœ€ä½æ™‚çš„æ¨¡å‹ç‹€æ…‹
    if best_model_state:
        model.load_state_dict(best_model_state)

    # å„²å­˜æ¨¡å‹ - ä¿å­˜è¨“ç·´å®Œæˆçš„æ¨¡å‹æ¬Šé‡
    torch.save(model.state_dict(), output_path)
    print(f"âœ… æ”¹è‰¯ç‰ˆæ¨¡å‹å·²è¨“ç·´ä¸¦å„²å­˜è‡³ {output_path}")

    # å›å‚³è¨“ç·´å®Œæˆçš„æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨
    return model, scaler

def classify_depreciation_enhanced_mlp(metrics, mlp_model, scaler):
    """ä½¿ç”¨æ”¹è‰¯ç‰ˆ MLP æ¨¡å‹é€²è¡Œåˆ†é¡ï¼Œè‡ªå‹•é©æ‡‰ç‰¹å¾µæ•¸é‡"""
    # æ ¹æ“š scaler çš„ç‰¹å¾µæ•¸é‡æ±ºå®šä½¿ç”¨å“ªäº›ç‰¹å¾µ - æª¢æŸ¥æ¨™æº–åŒ–å™¨æ”¯æ´çš„ç‰¹å¾µç¶­åº¦
    scaler_features = scaler.n_features_in_

    # æ ¹æ“šæ¨™æº–åŒ–å™¨çš„ç‰¹å¾µæ•¸é‡é¸æ“‡å°æ‡‰çš„ç‰¹å¾µé›†
    if scaler_features == 4:
        # åªä½¿ç”¨åŸºç¤ç‰¹å¾µ - 4å€‹æ ¸å¿ƒæŒ‡æ¨™
        features = [
            metrics["defect_index"], metrics["avg_depth"],   # æŠ˜èˆŠæŒ‡æ•¸å’Œå¹³å‡æ·±åº¦
            metrics["max_depth"], metrics["total_area"]      # æœ€å¤§æ·±åº¦å’Œç¸½é¢ç©
        ]
    else:
        # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾µ - åŒ…å«åŸºç¤ç‰¹å¾µå’Œé¡å¤–çµ±è¨ˆç‰¹å¾µ
        features = [
            metrics["defect_index"], metrics["avg_depth"],   # åŸºç¤ç‰¹å¾µï¼šæŠ˜èˆŠæŒ‡æ•¸å’Œå¹³å‡æ·±åº¦
            metrics["max_depth"], metrics["total_area"],     # åŸºç¤ç‰¹å¾µï¼šæœ€å¤§æ·±åº¦å’Œç¸½é¢ç©
            metrics.get("depth_std", 0), metrics.get("area_ratio", 0),   # é¡å¤–ç‰¹å¾µï¼šæ·±åº¦æ¨™æº–å·®å’Œé¢ç©æ¯”ä¾‹
            metrics.get("defect_density", 0)                 # é¡å¤–ç‰¹å¾µï¼šç¼ºé™·å¯†åº¦
        ]

    # å…¶é¤˜é‚è¼¯ä¿æŒä¸è®Š - ç‰¹å¾µæ¨™æº–åŒ–å’Œæ¨¡å‹æ¨è«–
    features_scaled = scaler.transform([features])          # ä½¿ç”¨æ¨™æº–åŒ–å™¨å°ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–
    input_tensor = torch.tensor(features_scaled, dtype=torch.float32)  # è½‰æ›ç‚º PyTorch å¼µé‡

    # åœç”¨æ¢¯åº¦è¨ˆç®—é€²è¡Œæ¨è«–
    with torch.no_grad():
        """Logits è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒï¼šä½¿ç”¨ torch.softmax(logits, dim=1) å°‡æ¨¡å‹çš„åŸå§‹è¼¸å‡º (logits) è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒï¼Œç¢ºä¿æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡ç¸½å’Œç‚º 1
           å–å¾—é æ¸¬é¡åˆ¥ï¼šä½¿ç”¨ torch.argmax(probs, dim=1).item() æ‰¾å‡ºæ©Ÿç‡æœ€é«˜çš„é¡åˆ¥ç´¢å¼•
           æå–ä¿¡å¿ƒåº¦ï¼šconfidence = probs[0, pred].item() å–å¾—è©²é æ¸¬é¡åˆ¥çš„æ©Ÿç‡å€¼ä½œç‚ºä¿¡å¿ƒåº¦
           ä¾‹å¦‚ï¼Œå¦‚æœæ©Ÿç‡åˆ†å¸ƒç‚º [0.12, 0.82, 0.06]ï¼Œé æ¸¬é¡åˆ¥ç‚ºç´¢å¼• 1 (Bç´š)ï¼Œå‰‡ä¿¡å¿ƒåº¦ç‚º 0.82"""
        logits = mlp_model(input_tensor)                    # æ¨¡å‹å‰å‘å‚³æ’­ï¼Œå–å¾—åŸå§‹è¼¸å‡º
        probs = torch.softmax(logits, dim=1)                # å°‡ logits è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒ
        pred = torch.argmax(probs, dim=1).item()            # å–å¾—é æ¸¬é¡åˆ¥ç´¢å¼•
        confidence = probs[0, pred].item()                  # å–å¾—é æ¸¬é¡åˆ¥çš„ä¿¡å¿ƒåº¦

        # è¨ˆç®—é æ¸¬ä¸ç¢ºå®šæ€§ï¼ˆåŸºæ–¼ç†µï¼‰
        """ä¸ç¢ºå®šæ€§åŸºæ–¼è³‡è¨Šç†µ (entropy) ä¾†è¡¡é‡æ¨¡å‹é æ¸¬çš„ä¸ç¢ºå®šç¨‹åº¦ï¼š
            è¨ˆç®—ç†µå€¼ï¼šentropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).item()
            ä½¿ç”¨è³‡è¨Šç†µå…¬å¼ï¼šH = -Î£(p_i * log(p_i))
            åŠ å…¥ 1e-8 é¿å… log(0) çš„æ•¸å€¼éŒ¯èª¤
            æ­£è¦åŒ–ä¸ç¢ºå®šæ€§ï¼šuncertainty = entropy / np.log(3)
            é™¤ä»¥æœ€å¤§å¯èƒ½ç†µå€¼ (log(3)ï¼Œå› ç‚ºæœ‰ 3 å€‹é¡åˆ¥) é€²è¡Œæ­£è¦åŒ–
            çµæœç¯„åœåœ¨ 0-1 ä¹‹é–“ï¼Œ1 è¡¨ç¤ºæœ€å¤§ä¸ç¢ºå®šæ€§ """
        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).item()  # è¨ˆç®—ç†µå€¼
        uncertainty = entropy / np.log(3)                  # æ­£è¦åŒ–ä¸ç¢ºå®šæ€§ï¼ˆé™¤ä»¥æœ€å¤§ç†µå€¼ï¼‰

    # é¡åˆ¥æ¨™ç±¤å°æ‡‰
    labels = ["A - normal", "B - Under_observation", "C - Recommended_repair"]
    # å›å‚³é æ¸¬ç­‰ç´šã€ä¿¡å¿ƒåº¦å’Œä¸ç¢ºå®šæ€§
    return labels[pred], confidence, uncertainty

def generate_enhanced_depreciation_record(defects, mlp_model=None, scaler=None, image_shape=(256, 256)):
    """ç”Ÿæˆå¢å¼·ç‰ˆæŠ˜èˆŠåˆ†æè¨˜éŒ„ï¼Œè‡ªå‹•é©æ‡‰ç‰¹å¾µæ•¸é‡
    ç•¶æœ‰è¨“ç·´å¥½çš„ MLP æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨æ™‚ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ï¼Œå¦å‰‡å›é€€åˆ°åŸºæ–¼è¦å‰‡çš„åˆ†é¡ã€‚
    å‡½æ•¸æœƒæ ¹æ“š scaler.n_features_in_ è‡ªå‹•é©æ‡‰ä½¿ç”¨ 4 å€‹åŸºç¤ç‰¹å¾µæˆ– 7 å€‹å®Œæ•´ç‰¹å¾µé›†ï¼Œç¢ºä¿èˆ‡ä¸åŒç‰ˆæœ¬çš„è¨“ç·´æ¨¡å‹ç›¸å®¹ã€‚
    defect_index - æŠ˜èˆŠæŒ‡æ•¸ï¼ˆé¢ç© Ã— æ·±åº¦çš„åŠ ç¸½ï¼‰
    avg_depth - å¹³å‡æ·±åº¦
    max_depth - æœ€å¤§æ·±åº¦
    total_area - æ‰€æœ‰ç¼ºé™·çš„ç¸½é¢ç©
    é¡å¤–:
    depth_std - æ·±åº¦æ¨™æº–å·®ï¼ˆè¡¡é‡æ·±åº¦è®Šç•°æ€§ï¼‰
    area_ratio - é¢ç©æ¯”ä¾‹ï¼ˆç¼ºé™·ç¸½é¢ç© / å½±åƒç¸½é¢ç©ï¼‰
    defect_density - ç¼ºé™·å¯†åº¦ï¼ˆæ¯è¬åƒç´ çš„ç¼ºé™·æ•¸é‡ï¼‰
    """
    # æ ¹æ“šæ˜¯å¦æœ‰ scaler æ±ºå®šç‰¹å¾µé¡å‹ - åˆ¤æ–·æ˜¯å¦ä½¿ç”¨åŸºç¤ç‰¹å¾µæˆ–å®Œæ•´ç‰¹å¾µé›†
    use_basic_only = scaler is None or scaler.n_features_in_ == 4
    # è¨ˆç®—å¢å¼·ç‰ˆæŠ˜èˆŠæŒ‡æ¨™ï¼Œæ ¹æ“š use_basic_only æ±ºå®šç‰¹å¾µæ•¸é‡
    metrics = compute_enhanced_depreciation_metrics(defects, image_shape, use_basic_features_only=use_basic_only)
    # å¦‚æœæœ‰ MLP æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨ï¼Œä½¿ç”¨æ©Ÿå™¨å­¸ç¿’åˆ†é¡
    if mlp_model and scaler:
        # ä½¿ç”¨å¢å¼·ç‰ˆ MLP æ¨¡å‹é€²è¡Œåˆ†é¡ï¼Œå›å‚³ç­‰ç´šã€ä¿¡å¿ƒåº¦å’Œä¸ç¢ºå®šæ€§
        grade, confidence, uncertainty = classify_depreciation_enhanced_mlp(metrics, mlp_model, scaler)
    else:
        # å¦å‰‡ä½¿ç”¨è¦å‰‡å¼åˆ†é¡æ–¹æ³•
        from train_depreciation_mlp import classify_depreciation
        # åƒ…ä½¿ç”¨æŠ˜èˆŠæŒ‡æ•¸é€²è¡Œç°¡å–®åˆ†ç´š
        grade = classify_depreciation(metrics["defect_index"])
        # è¦å‰‡å¼æ–¹æ³•ç„¡æ³•æä¾›ä¿¡å¿ƒåº¦å’Œä¸ç¢ºå®šæ€§
        confidence = "N/A"
        uncertainty = "N/A"

    # ç”¢ç”Ÿç•¶å‰æ™‚é–“æˆ³è¨˜ï¼Œæ ¼å¼ç‚º YYYY-MM-DD HH:MM
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    # å›å‚³å®Œæ•´çš„æŠ˜èˆŠåˆ†æè¨˜éŒ„å­—å…¸
    return {
        "timestamp": timestamp,        # åˆ†ææ™‚é–“
        "grade": grade,               # æŠ˜èˆŠç­‰ç´š (A/B/C)
        "confidence": confidence,     # æ¨¡å‹ä¿¡å¿ƒåº¦
        "uncertainty": uncertainty,   # é æ¸¬ä¸ç¢ºå®šæ€§
        **metrics,                   # å±•é–‹æ‰€æœ‰è¨ˆç®—çš„æŒ‡æ¨™
        "defects": defects           # åŸå§‹ç¼ºé™·æ¸…å–®
    }