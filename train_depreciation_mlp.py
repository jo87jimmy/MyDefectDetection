import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from datetime import datetime
import os
import torch.optim as optim
from torch.optim.lr_scheduler import ReduceLROnPlateau
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_class_weight
import pickle
import json


def classify_depreciation(defect_index):
    """
    ğŸ“Œ ç”¨é€”ï¼šæ ¹æ“šæŠ˜èˆŠæŒ‡æ•¸(defect_index)é€²è¡Œåˆ†ç´šåˆ¤æ–·ï¼Œå”åŠ©ä½¿ç”¨è€…å¿«é€Ÿäº†è§£ç‰©ä»¶ç›®å‰çš„å¥åº·ç‹€æ…‹ã€‚
    ğŸ“Š åˆ†ç´šé‚è¼¯ï¼š
    - Aï¼šnormal(defect_index < 50)
    - Bï¼šUnder_observation(50 â‰¤ defect_index < 150)
    - Cï¼šRecommended_repair(defect_index â‰¥ 150)
    ğŸ” å›å‚³ï¼š
    å°æ‡‰çš„æŠ˜èˆŠç­‰ç´šå­—ä¸²(å«å»ºè­°)
    """
    """åŸºæ–¼ä½ æ•¸æ“šçš„25%å’Œ75%ç™¾åˆ†ä½æ•¸ï¼Œèƒ½ç¢ºä¿ï¼š
        ç´„25%çš„æ•¸æ“šç‚ºAç´š(normal)
        ç´„50%çš„æ•¸æ“šç‚ºBç´š(Under_observation)
        ç´„25%çš„æ•¸æ“šç‚ºCç´š(Recommended_repair)
        å…¶ä»–è€ƒæ…®çš„æ–¹æ¡ˆï¼š
        æ¨™æº–å·®æ³•ï¼šA<2971, B<6225, Câ‰¥6225
        ä¿å®ˆæ³•ï¼šA<4631, B<6741, Câ‰¥6741 """
    """"é›–ç„¶è¦å‰‡å¼åˆ†é¡åœ¨åˆæœŸå¾ˆå¯¦ç”¨ï¼Œä½†éš¨è‘—æ•¸æ“šç´¯ç©ï¼Œ
    æ©Ÿå™¨å­¸ç¿’æ–¹æ³•é€šå¸¸èƒ½æä¾›æ›´å¥½çš„åˆ†é¡æº–ç¢ºæ€§å’Œé©æ‡‰æ€§ã€‚
    MyDefectDetection ç³»çµ±çš„æ··åˆæ¶æ§‹è¨­è¨ˆé«”ç¾äº†é€™ç¨®æ¼¸é€²å¼æ”¹é€²çš„æœ€ä½³å¯¦è¸ã€‚"""

    if defect_index < 2000:
        return "A - normal"
    elif defect_index < 3000:
        return "B - Under_observation"
    else:
        return "C - Recommended_repair"


class EnhancedDepreciationMLP(nn.Module):
    """æ”¹è‰¯ç‰ˆæŠ˜èˆŠåˆ†æ MLP æ¨¡å‹
    - æ›´æ·±çš„ç¶²è·¯æ¶æ§‹ï¼ˆ3å±¤éš±è—å±¤ï¼‰
    - Dropout é˜²éæ“¬åˆ
    - BatchNorm ç©©å®šè¨“ç·´
    - æ”¯æ´æ›´å¤šç‰¹å¾µè¼¸å…¥
    """

    def __init__(self,
                 input_dim=8,
                 hidden_dims=[64, 32, 16],
                 output_dim=3,
                 dropout_rate=0.3):
        super().__init__()

        layers = []
        prev_dim = input_dim

        # å»ºç«‹å¤šå±¤éš±è—å±¤
        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.LeakyReLU(0.1),
                nn.Dropout(dropout_rate)
            ])
            prev_dim = hidden_dim

        # è¼¸å‡ºå±¤
        layers.append(nn.Linear(prev_dim, output_dim))

        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)


def compute_enhanced_depreciation_metrics(defects,
                                          image_shape=(256, 256),
                                          use_basic_features_only=True):
    """è¨ˆç®—å¢å¼·ç‰ˆæŠ˜èˆŠæŒ‡æ¨™ï¼Œå¯é¸æ“‡åªä½¿ç”¨åŸºç¤ç‰¹å¾µ"""
    # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºé™·è³‡æ–™ï¼Œè‹¥ç„¡å‰‡å›å‚³é›¶å€¼æŒ‡æ¨™
    if not defects:
        # å»ºç«‹åŸºç¤æŒ‡æ¨™çš„é›¶å€¼å­—å…¸
        basic_metrics = {
            "defect_index": 0,
            "defect_count": 0,
            "avg_depth": 0,
            "max_depth": 0,
            "total_area": 0
        }
        # æ ¹æ“šåƒæ•¸æ±ºå®šæ˜¯å¦åªå›å‚³åŸºç¤ç‰¹å¾µ
        if use_basic_features_only:
            return basic_metrics
        else:
            # å›å‚³åŒ…å«é¡å¤–ç‰¹å¾µçš„å®Œæ•´é›¶å€¼å­—å…¸
            return {
                **basic_metrics, "depth_std": 0,
                "area_ratio": 0,
                "defect_density": 0
            }

    # åŸºç¤æŒ‡æ¨™è¨ˆç®—ä¿æŒä¸è®Š - è¨ˆç®—æ ¸å¿ƒæŠ˜èˆŠæŒ‡æ¨™
    defect_index = sum([d['area'] * d['depth']
                        for d in defects])  # è¨ˆç®—æŠ˜èˆŠæŒ‡æ•¸ï¼ˆé¢ç©Ã—æ·±åº¦çš„ç¸½å’Œï¼‰
    depths = [d['depth'] for d in defects]  # æå–æ‰€æœ‰ç¼ºé™·çš„æ·±åº¦å€¼
    areas = [d['area'] for d in defects]  # æå–æ‰€æœ‰ç¼ºé™·çš„é¢ç©å€¼

    avg_depth = np.mean(depths)  # è¨ˆç®—å¹³å‡æ·±åº¦
    max_depth = np.max(depths)  # æ‰¾å‡ºæœ€å¤§æ·±åº¦
    total_area = sum(areas)  # è¨ˆç®—ç¸½é¢ç©

    # çµ„å»ºåŸºç¤æŒ‡æ¨™å­—å…¸
    basic_metrics = {
        "defect_index": defect_index,  # æŠ˜èˆŠæŒ‡æ•¸
        "defect_count": len(defects),  # ç¼ºé™·æ•¸é‡
        "avg_depth": avg_depth,  # å¹³å‡æ·±åº¦
        "max_depth": max_depth,  # æœ€å¤§æ·±åº¦
        "total_area": total_area  # ç¸½é¢ç©
    }

    # å¦‚æœåªéœ€è¦åŸºç¤ç‰¹å¾µï¼Œç›´æ¥å›å‚³
    if use_basic_features_only:
        return basic_metrics

    # é¡å¤–ç‰¹å¾µ - è¨ˆç®—é€²éšçµ±è¨ˆç‰¹å¾µ
    depth_std = np.std(depths) if len(depths) > 1 else 0  # è¨ˆç®—æ·±åº¦æ¨™æº–å·®ï¼ˆéœ€å¤šæ–¼1å€‹ç¼ºé™·ï¼‰
    total_image_area = image_shape[0] * image_shape[1]  # è¨ˆç®—å½±åƒç¸½é¢ç©
    area_ratio = total_area / total_image_area  # è¨ˆç®—é¢ç©æ¯”ä¾‹ï¼ˆç¼ºé™·é¢ç©/å½±åƒé¢ç©ï¼‰
    defect_density = len(
        defects) / total_image_area * 10000  # è¨ˆç®—ç¼ºé™·å¯†åº¦ï¼ˆæ¯è¬åƒç´ çš„ç¼ºé™·æ•¸ï¼‰

    # å›å‚³åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„å®Œæ•´å­—å…¸
    return {
        **basic_metrics,  # å±•é–‹åŸºç¤æŒ‡æ¨™
        "depth_std": depth_std,  # æ·±åº¦æ¨™æº–å·®
        "area_ratio": area_ratio,  # é¢ç©æ¯”ä¾‹
        "defect_density": defect_density  # ç¼ºé™·å¯†åº¦
    }


def train_enhanced_mlp_from_csv(csv_path="depreciation_records.csv",
                                output_path="enhanced_depreciation_mlp.pth",
                                scaler_path="feature_scaler.pkl",
                                epochs=100,
                                patience=10):
    """æ”¹è‰¯ç‰ˆ MLP è¨“ç·´å‡½æ•¸
    - æ•¸æ“šæ¨™æº–åŒ–
    - è¨“ç·´/é©—è­‰åˆ†å‰²
    - æ—©åœæ©Ÿåˆ¶
    - å­¸ç¿’ç‡èª¿åº¦
    - é¡åˆ¥æ¬Šé‡å¹³è¡¡
    """

    # è®€å–æ•¸æ“š - å¾ CSV æª”æ¡ˆè¼‰å…¥æŠ˜èˆŠåˆ†æè¨˜éŒ„
    df = pd.read_csv(csv_path)

    # ç‰¹å¾µé¸æ“‡ï¼ˆæ”¯æ´æ›´å¤šç‰¹å¾µï¼‰- å…ˆè¨­å®šåŸºç¤ç‰¹å¾µé›†
    feature_cols = ["defect_index", "avg_depth", "max_depth", "total_area"]
    # æª¢æŸ¥æ˜¯å¦æœ‰é¡å¤–ç‰¹å¾µæ¬„ä½ï¼Œè‹¥æœ‰å‰‡æ“´å±•ç‰¹å¾µé›†
    if "depth_std" in df.columns:
        feature_cols.extend(["depth_std", "area_ratio", "defect_density"])

    # æå–ç‰¹å¾µçŸ©é™£å’Œæ¨™ç±¤å‘é‡
    X = df[feature_cols].values  # ç‰¹å¾µæ•¸æ“š
    # å°‡ç­‰ç´šå­—ä¸²æ˜ å°„ç‚ºæ•¸å€¼æ¨™ç±¤ (A=0, B=1, C=2)
    y = df["grade"].map({
        "A - normal": 0,
        "B - Under_observation": 1,
        "C - Recommended_repair": 2
    }).values

    # æ•¸æ“šæ¨™æº–åŒ– - å»ºç«‹æ¨™æº–åŒ–å™¨ä¸¦å°ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)  # è¨ˆç®—å‡å€¼å’Œæ¨™æº–å·®ä¸¦æ¨™æº–åŒ–

    # å„²å­˜æ¨™æº–åŒ–å™¨ - ä¿å­˜ä»¥ä¾›å¾ŒçºŒæ¨è«–ä½¿ç”¨
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)

    # è¨“ç·´/é©—è­‰åˆ†å‰² - åˆ†å±¤æŠ½æ¨£ç¢ºä¿å„é¡åˆ¥æ¯”ä¾‹ä¸€è‡´
    X_train, X_val, y_train, y_val = train_test_split(X_scaled,
                                                      y,
                                                      test_size=0.2,
                                                      random_state=42,
                                                      stratify=y)

    # è½‰æ›ç‚ºå¼µé‡ - å°‡ NumPy é™£åˆ—è½‰æ›ç‚º PyTorch å¼µé‡
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)  # è¨“ç·´ç‰¹å¾µ
    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # è¨“ç·´æ¨™ç±¤
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)  # é©—è­‰ç‰¹å¾µ
    y_val_tensor = torch.tensor(y_val, dtype=torch.long)  # é©—è­‰æ¨™ç±¤

    # è¨ˆç®—é¡åˆ¥æ¬Šé‡ - è™•ç†é¡åˆ¥ä¸å¹³è¡¡å•é¡Œ
    class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)

    # å»ºç«‹æ¨¡å‹ - æ ¹æ“šç‰¹å¾µæ•¸é‡è¨­å®šè¼¸å…¥ç¶­åº¦
    model = EnhancedDepreciationMLP(input_dim=len(feature_cols))

    # è¼‰å…¥å·²å­˜åœ¨çš„æ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰- æ”¯æ´å¢é‡è¨“ç·´
    if os.path.exists(output_path):
        print(f"ğŸ“‚ è¼‰å…¥å·²å­˜åœ¨æ¨¡å‹ {output_path}")
        model.load_state_dict(torch.load(output_path, weights_only=True))

    # è¨“ç·´è¨­å®š - é…ç½®æå¤±å‡½æ•¸ã€å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨
    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)  # åŠ æ¬Šäº¤å‰ç†µæå¤±
    optimizer = optim.AdamW(model.parameters(), lr=0.001,
                            weight_decay=0.01)  # AdamW å„ªåŒ–å™¨
    scheduler = ReduceLROnPlateau(optimizer,
                                  mode='min',
                                  factor=0.5,
                                  patience=5,
                                  verbose=True)  # å­¸ç¿’ç‡èª¿åº¦

    # æ—©åœè¨­å®š - é˜²æ­¢éæ“¬åˆ
    best_val_loss = float('inf')  # è¨˜éŒ„æœ€ä½³é©—è­‰æå¤±
    patience_counter = 0  # è€å¿ƒè¨ˆæ•¸å™¨
    best_model_state = None  # æœ€ä½³æ¨¡å‹ç‹€æ…‹

    # è¨“ç·´è¿´åœˆ - å»ºç«‹æ•¸æ“šè¼‰å…¥å™¨ä¸¦é–‹å§‹è¨“ç·´
    train_dataset = torch.utils.data.TensorDataset(X_train_tensor,
                                                   y_train_tensor)
    train_loader = torch.utils.data.DataLoader(train_dataset,
                                               batch_size=32,
                                               shuffle=True)

    model.train()  # è¨­å®šç‚ºè¨“ç·´æ¨¡å¼
    for epoch in range(epochs):
        # è¨“ç·´éšæ®µ - æ‰¹æ¬¡è¨“ç·´
        train_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦
            logits = model(batch_x)  # å‰å‘å‚³æ’­
            loss = criterion(logits, batch_y)  # è¨ˆç®—æå¤±
            loss.backward()  # åå‘å‚³æ’­
            optimizer.step()  # æ›´æ–°åƒæ•¸
            train_loss += loss.item()  # ç´¯ç©æå¤±

        # é©—è­‰éšæ®µ - è©•ä¼°æ¨¡å‹æ€§èƒ½
        model.eval()  # è¨­å®šç‚ºè©•ä¼°æ¨¡å¼
        with torch.no_grad():  # åœç”¨æ¢¯åº¦è¨ˆç®—
            val_logits = model(X_val_tensor)  # é©—è­‰é›†å‰å‘å‚³æ’­
            val_loss = criterion(val_logits, y_val_tensor).item()  # è¨ˆç®—é©—è­‰æå¤±
            val_acc = (torch.argmax(
                val_logits,
                dim=1) == y_val_tensor).float().mean().item()  # è¨ˆç®—æº–ç¢ºç‡

        # å­¸ç¿’ç‡èª¿åº¦ - æ ¹æ“šé©—è­‰æå¤±èª¿æ•´å­¸ç¿’ç‡
        scheduler.step(val_loss)

        # æ—©åœæª¢æŸ¥ - ç›£æ§é©—è­‰æå¤±æ”¹å–„æƒ…æ³
        if val_loss < best_val_loss:
            best_val_loss = val_loss  # æ›´æ–°æœ€ä½³æå¤±
            patience_counter = 0  # é‡ç½®è€å¿ƒè¨ˆæ•¸å™¨
            best_model_state = model.state_dict().copy()  # ä¿å­˜æœ€ä½³æ¨¡å‹ç‹€æ…‹
        else:
            patience_counter += 1  # å¢åŠ è€å¿ƒè¨ˆæ•¸å™¨

        # æ¯ 10 å€‹ epoch è¼¸å‡ºè¨“ç·´é€²åº¦
        if epoch % 10 == 0:
            print(
                f"Epoch {epoch}: Train Loss={train_loss/len(train_loader):.4f}, "
                f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.4f}")

        # æ—©åœæ¢ä»¶æª¢æŸ¥ - è‹¥é©—è­‰æå¤±é•·æ™‚é–“æœªæ”¹å–„å‰‡åœæ­¢è¨“ç·´
        if patience_counter >= patience:
            print(f"æ—©åœæ–¼ epoch {epoch}")
            break

        model.train()  # å›åˆ°è¨“ç·´æ¨¡å¼

    # è¼‰å…¥æœ€ä½³æ¨¡å‹ - æ¢å¾©é©—è­‰æå¤±æœ€ä½æ™‚çš„æ¨¡å‹ç‹€æ…‹
    if best_model_state:
        model.load_state_dict(best_model_state)

    # å„²å­˜æ¨¡å‹ - ä¿å­˜è¨“ç·´å®Œæˆçš„æ¨¡å‹æ¬Šé‡
    torch.save(model.state_dict(), output_path)
    print(f"âœ… æ”¹è‰¯ç‰ˆæ¨¡å‹å·²è¨“ç·´ä¸¦å„²å­˜è‡³ {output_path}")

    # å›å‚³è¨“ç·´å®Œæˆçš„æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨
    return model, scaler


def classify_depreciation_enhanced_mlp(metrics, mlp_model, scaler):
    """ä½¿ç”¨æ”¹è‰¯ç‰ˆ MLP æ¨¡å‹é€²è¡Œåˆ†é¡ï¼Œè‡ªå‹•é©æ‡‰ç‰¹å¾µæ•¸é‡"""
    # æ ¹æ“š scaler çš„ç‰¹å¾µæ•¸é‡æ±ºå®šä½¿ç”¨å“ªäº›ç‰¹å¾µ - æª¢æŸ¥æ¨™æº–åŒ–å™¨æ”¯æ´çš„ç‰¹å¾µç¶­åº¦
    scaler_features = scaler.n_features_in_

    # æ ¹æ“šæ¨™æº–åŒ–å™¨çš„ç‰¹å¾µæ•¸é‡é¸æ“‡å°æ‡‰çš„ç‰¹å¾µé›†
    if scaler_features == 4:
        # åªä½¿ç”¨åŸºç¤ç‰¹å¾µ - 4å€‹æ ¸å¿ƒæŒ‡æ¨™
        features = [
            metrics["defect_index"],
            metrics["avg_depth"],  # æŠ˜èˆŠæŒ‡æ•¸å’Œå¹³å‡æ·±åº¦
            metrics["max_depth"],
            metrics["total_area"]  # æœ€å¤§æ·±åº¦å’Œç¸½é¢ç©
        ]
    else:
        # ä½¿ç”¨æ‰€æœ‰ç‰¹å¾µ - åŒ…å«åŸºç¤ç‰¹å¾µå’Œé¡å¤–çµ±è¨ˆç‰¹å¾µ
        features = [
            metrics["defect_index"],
            metrics["avg_depth"],  # åŸºç¤ç‰¹å¾µï¼šæŠ˜èˆŠæŒ‡æ•¸å’Œå¹³å‡æ·±åº¦
            metrics["max_depth"],
            metrics["total_area"],  # åŸºç¤ç‰¹å¾µï¼šæœ€å¤§æ·±åº¦å’Œç¸½é¢ç©
            metrics.get("depth_std", 0),
            metrics.get("area_ratio", 0),  # é¡å¤–ç‰¹å¾µï¼šæ·±åº¦æ¨™æº–å·®å’Œé¢ç©æ¯”ä¾‹
            metrics.get("defect_density", 0)  # é¡å¤–ç‰¹å¾µï¼šç¼ºé™·å¯†åº¦
        ]

    # å…¶é¤˜é‚è¼¯ä¿æŒä¸è®Š - ç‰¹å¾µæ¨™æº–åŒ–å’Œæ¨¡å‹æ¨è«–
    features_scaled = scaler.transform([features])  # ä½¿ç”¨æ¨™æº–åŒ–å™¨å°ç‰¹å¾µé€²è¡Œæ¨™æº–åŒ–
    input_tensor = torch.tensor(features_scaled,
                                dtype=torch.float32)  # è½‰æ›ç‚º PyTorch å¼µé‡

    # åœç”¨æ¢¯åº¦è¨ˆç®—é€²è¡Œæ¨è«–
    with torch.no_grad():
        """Logits è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒï¼šä½¿ç”¨ torch.softmax(logits, dim=1) å°‡æ¨¡å‹çš„åŸå§‹è¼¸å‡º (logits) è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒï¼Œç¢ºä¿æ‰€æœ‰é¡åˆ¥çš„æ©Ÿç‡ç¸½å’Œç‚º 1
           å–å¾—é æ¸¬é¡åˆ¥ï¼šä½¿ç”¨ torch.argmax(probs, dim=1).item() æ‰¾å‡ºæ©Ÿç‡æœ€é«˜çš„é¡åˆ¥ç´¢å¼•
           æå–ä¿¡å¿ƒåº¦ï¼šconfidence = probs[0, pred].item() å–å¾—è©²é æ¸¬é¡åˆ¥çš„æ©Ÿç‡å€¼ä½œç‚ºä¿¡å¿ƒåº¦
           ä¾‹å¦‚ï¼Œå¦‚æœæ©Ÿç‡åˆ†å¸ƒç‚º [0.12, 0.82, 0.06]ï¼Œé æ¸¬é¡åˆ¥ç‚ºç´¢å¼• 1 (Bç´š)ï¼Œå‰‡ä¿¡å¿ƒåº¦ç‚º 0.82"""
        logits = mlp_model(input_tensor)  # æ¨¡å‹å‰å‘å‚³æ’­ï¼Œå–å¾—åŸå§‹è¼¸å‡º
        probs = torch.softmax(logits, dim=1)  # å°‡ logits è½‰æ›ç‚ºæ©Ÿç‡åˆ†å¸ƒ
        pred = torch.argmax(probs, dim=1).item()  # å–å¾—é æ¸¬é¡åˆ¥ç´¢å¼•
        confidence = probs[0, pred].item()  # å–å¾—é æ¸¬é¡åˆ¥çš„ä¿¡å¿ƒåº¦

        # è¨ˆç®—é æ¸¬ä¸ç¢ºå®šæ€§ï¼ˆåŸºæ–¼ç†µï¼‰
        """ä¸ç¢ºå®šæ€§åŸºæ–¼è³‡è¨Šç†µ (entropy) ä¾†è¡¡é‡æ¨¡å‹é æ¸¬çš„ä¸ç¢ºå®šç¨‹åº¦ï¼š
            è¨ˆç®—ç†µå€¼ï¼šentropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).item()
            ä½¿ç”¨è³‡è¨Šç†µå…¬å¼ï¼šH = -Î£(p_i * log(p_i))
            åŠ å…¥ 1e-8 é¿å… log(0) çš„æ•¸å€¼éŒ¯èª¤
            æ­£è¦åŒ–ä¸ç¢ºå®šæ€§ï¼šuncertainty = entropy / np.log(3)
            é™¤ä»¥æœ€å¤§å¯èƒ½ç†µå€¼ (log(3)ï¼Œå› ç‚ºæœ‰ 3 å€‹é¡åˆ¥) é€²è¡Œæ­£è¦åŒ–
            çµæœç¯„åœåœ¨ 0-1 ä¹‹é–“ï¼Œ1 è¡¨ç¤ºæœ€å¤§ä¸ç¢ºå®šæ€§ """
        entropy = -torch.sum(probs * torch.log(probs + 1e-8),
                             dim=1).item()  # è¨ˆç®—ç†µå€¼
        uncertainty = entropy / np.log(3)  # æ­£è¦åŒ–ä¸ç¢ºå®šæ€§ï¼ˆé™¤ä»¥æœ€å¤§ç†µå€¼ï¼‰

    # é¡åˆ¥æ¨™ç±¤å°æ‡‰
    labels = ["A - normal", "B - Under_observation", "C - Recommended_repair"]
    # å›å‚³é æ¸¬ç­‰ç´šã€ä¿¡å¿ƒåº¦å’Œä¸ç¢ºå®šæ€§
    return labels[pred], confidence, uncertainty


def generate_enhanced_depreciation_record(defects,
                                          mlp_model=None,
                                          scaler=None,
                                          image_shape=(256, 256)):
    """ç”Ÿæˆå¢å¼·ç‰ˆæŠ˜èˆŠåˆ†æè¨˜éŒ„ï¼Œè‡ªå‹•é©æ‡‰ç‰¹å¾µæ•¸é‡
    ç•¶æœ‰è¨“ç·´å¥½çš„ MLP æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨æ™‚ä½¿ç”¨æ©Ÿå™¨å­¸ç¿’æ–¹æ³•ï¼Œå¦å‰‡å›é€€åˆ°åŸºæ–¼è¦å‰‡çš„åˆ†é¡ã€‚
    å‡½æ•¸æœƒæ ¹æ“š scaler.n_features_in_ è‡ªå‹•é©æ‡‰ä½¿ç”¨ 4 å€‹åŸºç¤ç‰¹å¾µæˆ– 7 å€‹å®Œæ•´ç‰¹å¾µé›†ï¼Œç¢ºä¿èˆ‡ä¸åŒç‰ˆæœ¬çš„è¨“ç·´æ¨¡å‹ç›¸å®¹ã€‚
    defect_index - æŠ˜èˆŠæŒ‡æ•¸ï¼ˆé¢ç© Ã— æ·±åº¦çš„åŠ ç¸½ï¼‰
    avg_depth - å¹³å‡æ·±åº¦
    max_depth - æœ€å¤§æ·±åº¦
    total_area - æ‰€æœ‰ç¼ºé™·çš„ç¸½é¢ç©
    é¡å¤–:
    depth_std - æ·±åº¦æ¨™æº–å·®ï¼ˆè¡¡é‡æ·±åº¦è®Šç•°æ€§ï¼‰
    area_ratio - é¢ç©æ¯”ä¾‹ï¼ˆç¼ºé™·ç¸½é¢ç© / å½±åƒç¸½é¢ç©ï¼‰
    defect_density - ç¼ºé™·å¯†åº¦ï¼ˆæ¯è¬åƒç´ çš„ç¼ºé™·æ•¸é‡ï¼‰
    """
    # æ ¹æ“šæ˜¯å¦æœ‰ scaler æ±ºå®šç‰¹å¾µé¡å‹ - åˆ¤æ–·æ˜¯å¦ä½¿ç”¨åŸºç¤ç‰¹å¾µæˆ–å®Œæ•´ç‰¹å¾µé›†
    use_basic_only = scaler is None or scaler.n_features_in_ == 4
    # use_basic_only = False #è¦æ‰‹å‹•æ”¹æˆ False æ‰æœƒç”¨åˆ°å®Œæ•´ç‰¹å¾µ
    print("use_basic_only:", use_basic_only)
    # è¨ˆç®—å¢å¼·ç‰ˆæŠ˜èˆŠæŒ‡æ¨™ï¼Œæ ¹æ“š use_basic_only æ±ºå®šç‰¹å¾µæ•¸é‡
    metrics = compute_enhanced_depreciation_metrics(
        defects, image_shape, use_basic_features_only=use_basic_only)
    # å¦‚æœæœ‰ MLP æ¨¡å‹å’Œæ¨™æº–åŒ–å™¨ï¼Œä½¿ç”¨æ©Ÿå™¨å­¸ç¿’åˆ†é¡
    if mlp_model and scaler:
        # ä½¿ç”¨å¢å¼·ç‰ˆ MLP æ¨¡å‹é€²è¡Œåˆ†é¡ï¼Œå›å‚³ç­‰ç´šã€ä¿¡å¿ƒåº¦å’Œä¸ç¢ºå®šæ€§
        grade, confidence, uncertainty = classify_depreciation_enhanced_mlp(
            metrics, mlp_model, scaler)
    else:
        # å¦å‰‡ä½¿ç”¨è¦å‰‡å¼åˆ†é¡æ–¹æ³•
        from train_depreciation_mlp import classify_depreciation
        # åƒ…ä½¿ç”¨æŠ˜èˆŠæŒ‡æ•¸é€²è¡Œç°¡å–®åˆ†ç´š
        grade = classify_depreciation(metrics["defect_index"])
        # è¦å‰‡å¼æ–¹æ³•ç„¡æ³•æä¾›ä¿¡å¿ƒåº¦å’Œä¸ç¢ºå®šæ€§
        confidence = "N/A"
        uncertainty = "N/A"

    # ç”¢ç”Ÿç•¶å‰æ™‚é–“æˆ³è¨˜ï¼Œæ ¼å¼ç‚º YYYY-MM-DD HH:MM
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")
    # å›å‚³å®Œæ•´çš„æŠ˜èˆŠåˆ†æè¨˜éŒ„å­—å…¸
    return {
        "timestamp": timestamp,  # åˆ†ææ™‚é–“
        "grade": grade,  # æŠ˜èˆŠç­‰ç´š (A/B/C)
        "confidence": confidence,  # æ¨¡å‹ä¿¡å¿ƒåº¦
        "uncertainty": uncertainty,  # é æ¸¬ä¸ç¢ºå®šæ€§
        **metrics,  # å±•é–‹æ‰€æœ‰è¨ˆç®—çš„æŒ‡æ¨™
        "defects": defects  # åŸå§‹ç¼ºé™·æ¸…å–®
    }


def generate_comparison_depreciation_record_with_excel(defects,
                                                       mlp_model=None,
                                                       scaler=None,
                                                       image_shape=(256, 256),
                                                       output_excel=True):
    """ç”Ÿæˆå°ç…§å¯¦é©—çš„æŠ˜èˆŠåˆ†æè¨˜éŒ„ï¼ŒåŒæ™‚æ¸¬è©¦ MLP å’Œè¦å‰‡å¼åˆ†é¡

    æ­¤å‡½æ•¸æœƒåŒæ™‚é‹è¡Œå…©ç¨®åˆ†é¡æ–¹æ³•ä¸¦è¨˜éŒ„çµæœå·®ç•°ï¼š
    1. MLP æ©Ÿå™¨å­¸ç¿’åˆ†é¡ï¼ˆå¦‚æœæ¨¡å‹å¯ç”¨ï¼‰
    2. è¦å‰‡å¼é–¾å€¼åˆ†é¡

    è¿”å›åŒ…å«å…©ç¨®æ–¹æ³•çµæœçš„å®Œæ•´æ¯”è¼ƒè¨˜éŒ„ ç”Ÿæˆå°ç…§å¯¦é©—è¨˜éŒ„ä¸¦è¼¸å‡º Excel æª”æ¡ˆ
    """

    from train_depreciation_mlp import classify_depreciation
    from datetime import datetime
    import pandas as pd
    import os

    # æ ¹æ“šæ˜¯å¦æœ‰ scaler æ±ºå®šç‰¹å¾µé¡å‹
    use_basic_only = scaler is None or scaler.n_features_in_ == 4

    # è¨ˆç®—å¢å¼·ç‰ˆæŠ˜èˆŠæŒ‡æ¨™
    metrics = compute_enhanced_depreciation_metrics(
        defects, image_shape, use_basic_features_only=use_basic_only)

    # === æ–¹æ³•1ï¼šMLP æ©Ÿå™¨å­¸ç¿’åˆ†é¡ ===
    mlp_result = {}
    if mlp_model and scaler:
        try:
            grade_mlp, confidence_mlp, uncertainty_mlp = classify_depreciation_enhanced_mlp(
                metrics, mlp_model, scaler)
            mlp_result = {
                "grade": grade_mlp,
                "confidence": confidence_mlp,
                "uncertainty": uncertainty_mlp,
                "method": "MLP",
                "available": True
            }
            print(
                f"ğŸ“Š MLP åˆ†æ - ç­‰ç´š: {grade_mlp}, ä¿¡å¿ƒ: {confidence_mlp:.3f}, ä¸ç¢ºå®šæ€§: {uncertainty_mlp:.3f}"
            )
        except Exception as e:
            mlp_result = {
                "grade": "ERROR",
                "confidence": "N/A",
                "uncertainty": "N/A",
                "method": "MLP",
                "available": False,
                "error": str(e)
            }
            print(f"âš ï¸ MLP åˆ†æå¤±æ•—: {e}")
    else:
        mlp_result = {
            "grade": "N/A",
            "confidence": "N/A",
            "uncertainty": "N/A",
            "method": "MLP",
            "available": False,
            "reason": "æ¨¡å‹æˆ–æ¨™æº–åŒ–å™¨ä¸å¯ç”¨"
        }
        print("âš ï¸ MLP æ¨¡å‹ä¸å¯ç”¨")

    # === æ–¹æ³•2ï¼šè¦å‰‡å¼é–¾å€¼åˆ†é¡ ===
    try:
        grade_rule = classify_depreciation(metrics["defect_index"])
        rule_result = {
            "grade": grade_rule,
            "confidence": "N/A",
            "uncertainty": "N/A",
            "method": "Rule-based",
            "available": True,
            "threshold_used": "3876/5554"
        }
        print(f"ğŸ“Š è¦å‰‡å¼åˆ†æ - ç­‰ç´š: {grade_rule}")
    except Exception as e:
        rule_result = {
            "grade": "ERROR",
            "confidence": "N/A",
            "uncertainty": "N/A",
            "method": "Rule-based",
            "available": False,
            "error": str(e)
        }
        print(f"âš ï¸ è¦å‰‡å¼åˆ†æå¤±æ•—: {e}")

    # === çµæœæ¯”è¼ƒåˆ†æ ===
    comparison = {
        "methods_agree":
        mlp_result["grade"] == rule_result["grade"]
        if mlp_result["available"] and rule_result["available"] else None,
        "grade_difference":
        None,
        "confidence_level":
        mlp_result.get("confidence", "N/A")
    }

    if mlp_result["available"] and rule_result["available"]:
        if mlp_result["grade"] != rule_result["grade"]:
            comparison[
                "grade_difference"] = f"{mlp_result['grade']} vs {rule_result['grade']}"
            print(
                f"âš ï¸ åˆ†é¡çµæœä¸ä¸€è‡´: MLP={mlp_result['grade']}, è¦å‰‡å¼={rule_result['grade']}"
            )
        else:
            print(f"âœ… å…©ç¨®æ–¹æ³•çµæœä¸€è‡´: {mlp_result['grade']}")

    # ç”¢ç”Ÿæ™‚é–“æˆ³è¨˜
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M")

    # å›å‚³å®Œæ•´çš„å°ç…§å¯¦é©—è¨˜éŒ„
    record = {
        "timestamp":
        timestamp,
        "experiment_type":
        "comparison",

        # ä¸»è¦çµæœ
        "grade":
        mlp_result["grade"]
        if mlp_result["available"] else rule_result["grade"],
        "confidence":
        mlp_result["confidence"],
        "uncertainty":
        mlp_result["uncertainty"],

        # MLP æ–¹æ³•çµæœ
        "mlp_grade":
        mlp_result["grade"],
        "mlp_confidence":
        mlp_result["confidence"],
        "mlp_uncertainty":
        mlp_result["uncertainty"],
        "mlp_available":
        mlp_result["available"],

        # è¦å‰‡å¼æ–¹æ³•çµæœ
        "rule_grade":
        rule_result["grade"],
        "rule_available":
        rule_result["available"],
        "rule_threshold":
        rule_result.get("threshold_used", "N/A"),

        # æ¯”è¼ƒåˆ†æ
        "methods_agree":
        comparison["methods_agree"],
        "grade_difference":
        comparison["grade_difference"],

        # åŸå§‹æŒ‡æ¨™å’Œç¼ºé™·æ•¸æ“š
        **metrics,
        "defects":
        defects
    }

    # å„²å­˜åˆ° CSVï¼ˆä½¿ç”¨ç¾æœ‰å‡½æ•¸ï¼‰
    save_record_to_csv(record, "comparison_records.csv")

    # # ç”¢å‡º Excel æª”æ¡ˆ
    # if output_excel:
    #     save_comparison_to_excel(record, "comparison_analysis.xlsx")

    return record


def interpret_comparison_results(record):
    """è§£é‡‹å°ç…§å¯¦é©—çµæœ"""

    print(f"ğŸ“Š å°ç…§å¯¦é©—çµæœè§£é‡‹ ({record['timestamp']})")
    print("=" * 50)

    # 1. åŸºæœ¬è³‡è¨Š
    print(f"æŠ˜èˆŠæŒ‡æ•¸: {record['defect_index']:.2f}")
    print(f"ç¼ºé™·æ•¸é‡: {record['defect_count']}")
    print(f"ç¸½é¢ç©: {record['total_area']:.1f}")

    # 2. æ–¹æ³•æ¯”è¼ƒ
    mlp_grade = record['mlp_grade']
    rule_grade = record['rule_grade']
    methods_agree = record['methods_agree']

    if methods_agree:
        print(f"âœ… å…©ç¨®æ–¹æ³•çµæœä¸€è‡´: {mlp_grade}")
        print("   â†’ åˆ†é¡çµæœå¯ä¿¡åº¦é«˜")
    elif methods_agree is False:
        print(f"âš ï¸ åˆ†é¡çµæœä¸ä¸€è‡´:")
        print(f"   MLP é æ¸¬: {mlp_grade}")
        print(f"   è¦å‰‡å¼: {rule_grade}")

        # 3. ä¿¡å¿ƒåº¦åˆ†æ
        confidence = record['mlp_confidence']
        if isinstance(confidence, (int, float)) and confidence >= 0.7:
            print(f"   â†’ MLP é«˜ä¿¡å¿ƒåº¦ ({confidence:.3f})ï¼Œå»ºè­°æ¡ç”¨ MLP çµæœ")
        else:
            print(f"   â†’ MLP ä¿¡å¿ƒåº¦è¼ƒä½ ({confidence})ï¼Œå»ºè­°äººå·¥è¤‡æ ¸")

    # 4. ä¸ç¢ºå®šæ€§è©•ä¼°
    uncertainty = record['mlp_uncertainty']
    if isinstance(uncertainty, (int, float)):
        if uncertainty < 0.3:
            print(f"ğŸ“ˆ ä½ä¸ç¢ºå®šæ€§ ({uncertainty:.3f})ï¼šé æ¸¬ç©©å®š")
        elif uncertainty > 0.7:
            print(f"ğŸ“‰ é«˜ä¸ç¢ºå®šæ€§ ({uncertainty:.3f})ï¼šå»ºè­°è¬¹æ…è™•ç†")

    # 5. å»ºè­°è¡Œå‹•
    print("\nğŸ’¡ å»ºè­°è¡Œå‹•:")
    if methods_agree and isinstance(
            record['mlp_confidence'],
        (int, float)) and record['mlp_confidence'] >= 0.8:
        print("   â†’ å¯è‡ªå‹•é€šéï¼Œç„¡éœ€äººå·¥ä»‹å…¥")
    elif not methods_agree or (isinstance(record['mlp_confidence'],
                                          (int, float))
                               and record['mlp_confidence'] < 0.6):
        print("   â†’ å»ºè­°äººå·¥è¤‡æ ¸ç¢ºèª")
    else:
        print("   â†’ å¯æ¥å—çµæœï¼Œä½†å»ºè­°å®šæœŸæŠ½æŸ¥")


def save_record_to_csv(record, csv_path="depreciation_records.csv"):
    """æ”¹è‰¯ç‰ˆ CSV å„²å­˜å‡½æ•¸ï¼Œç¢ºä¿æ•¸æ“šæ ¼å¼æ­£ç¢ºä¸¦è™•ç† NumPy é¡å‹"""
    record_copy = record.copy()

    if 'defects' in record_copy:
        # è½‰æ› NumPy é¡å‹ç‚º Python åŸç”Ÿé¡å‹
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj

        # æ¸…ç† defects æ•¸æ“šä¸¦è½‰ç‚º JSON
        cleaned_defects = convert_numpy_types(record_copy['defects'])
        record_copy['defects'] = json.dumps(cleaned_defects)

    # åŒæ™‚æ¸…ç†å…¶ä»–å¯èƒ½çš„ NumPy é¡å‹
    for key, value in record_copy.items():
        if isinstance(value, (np.integer, np.int64, np.int32)):
            record_copy[key] = int(value)
        elif isinstance(value, (np.floating, np.float64, np.float32)):
            record_copy[key] = float(value)

    df = pd.DataFrame([record_copy])

    try:
        if os.path.exists(csv_path):
            df.to_csv(csv_path, mode='a', header=False, index=False)
        else:
            df.to_csv(csv_path, index=False)
        print(f"âœ… è¨˜éŒ„å·²å„²å­˜è‡³ {csv_path}")
    except Exception as e:
        print(f"âš ï¸ å„²å­˜ CSV å¤±æ•—: {e}")


#é‚Šç•Œæ•ˆæ‡‰åˆ†æå‡½æ•¸
def analyze_boundary_effects(record, boundary_tolerance=200):
    """åˆ†ææ¥è¿‘é–¾å€¼é‚Šç•Œçš„æ¡ˆä¾‹è¡¨ç¾"""
    defect_index = record['defect_index']

    # å®šç¾©é‚Šç•Œå€é–“
    ab_boundary = 3876
    bc_boundary = 5554

    boundary_analysis = {
        "is_boundary_case": False,
        "boundary_type": None,
        "distance_to_boundary": None,
        "risk_level": "normal"
    }

    # æª¢æŸ¥æ˜¯å¦æ¥è¿‘ A/B é‚Šç•Œ
    if abs(defect_index - ab_boundary) <= boundary_tolerance:
        boundary_analysis.update({
            "is_boundary_case":
            True,
            "boundary_type":
            "A/B",
            "distance_to_boundary":
            abs(defect_index - ab_boundary),
            "risk_level":
            "high" if abs(defect_index - ab_boundary) < 100 else "medium"
        })

    # æª¢æŸ¥æ˜¯å¦æ¥è¿‘ B/C é‚Šç•Œ
    elif abs(defect_index - bc_boundary) <= boundary_tolerance:
        boundary_analysis.update({
            "is_boundary_case":
            True,
            "boundary_type":
            "B/C",
            "distance_to_boundary":
            abs(defect_index - bc_boundary),
            "risk_level":
            "high" if abs(defect_index - bc_boundary) < 100 else "medium"
        })

    return boundary_analysis
