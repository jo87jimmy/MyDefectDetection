import torch
import cv2
import numpy as np
from scipy.ndimage import gaussian_filter
import torch.nn.functional as F
from depreciation_analysis import generate_depreciation_record
from train_depreciation_mlp import train_mlp_from_csv

from train_depreciation_mlp import DepreciationMLP  # å…ˆæŠŠ class åŒ¯å…¥
class FullModel(torch.nn.Module):
    def __init__(self, encoder, bn, decoder):
        super().__init__()
        self.encoder = encoder
        self.bn = bn
        self.decoder = decoder

    def forward(self, x):
        feats = self.encoder(x)
        recons = self.decoder(self.bn(feats))
        return feats, recons
# ===== è¼”åŠ©å‡½å¼ =====
def min_max_norm(image):
    a_min, a_max = image.min(), image.max()
    return (image - a_min) / (a_max - a_min + 1e-8)

def cvt2heatmap(gray):
    heatmap = cv2.applyColorMap(np.uint8(gray), cv2.COLORMAP_JET)
    return heatmap

def show_cam_on_image(img, anomaly_map):
    cam = np.float32(anomaly_map) / 255 + np.float32(img) / 255
    cam = cam / np.max(cam)
    return np.uint8(255 * cam)

def cal_anomaly_map(fs_list, ft_list, out_size=224, amap_mode='a'):
    anomaly_map = np.zeros([out_size, out_size])
    a_map_list = []
    for fs, ft in zip(fs_list, ft_list):
        a_map = 1 - F.cosine_similarity(fs, ft)
        a_map = torch.unsqueeze(a_map, dim=1)
        a_map = F.interpolate(a_map, size=out_size, mode='bilinear', align_corners=True)
        a_map = a_map[0, 0, :, :].cpu().numpy()
        a_map_list.append(a_map)
        anomaly_map += a_map
    return anomaly_map, a_map_list

def extract_defect_regions(anomaly_map, threshold=0.6):
    norm_map = min_max_norm(anomaly_map)
    binary_mask = (norm_map > threshold).astype(np.uint8)

    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    defects = []
    for cnt in contours:
        area = cv2.contourArea(cnt)
        if area < 10:
            continue

        x, y, w, h = cv2.boundingRect(cnt)

        # å»ºç«‹é®ç½©ä¸¦æ‰¾æœ€æ·±é»ž
        mask = np.zeros_like(anomaly_map, dtype=np.uint8)
        cv2.drawContours(mask, [cnt], -1, 1, -1)
        masked_anomaly = np.where(mask == 1, anomaly_map, -np.inf)
        max_idx = np.unravel_index(np.argmax(masked_anomaly), anomaly_map.shape)
        cx, cy = max_idx[1], max_idx[0]  # æ³¨æ„ï¼šOpenCV æ˜¯ (x, y)

        depth = anomaly_map[max_idx]

        defects.append({
            "area": area,
            "center": (cx, cy),
            "size": (w, h),
            "depth": depth
        })

    return defects

def extract_and_annotate_defects(img, anomaly_map, threshold=0.6):
    smoothed_map = cv2.GaussianBlur(anomaly_map, (5, 5), sigmaX=2)
    norm_map = min_max_norm(smoothed_map)
    binary_mask = (norm_map > threshold).astype(np.uint8) * 255

    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    annotated = img.copy()
    for i, cnt in enumerate(contours):
        area = cv2.contourArea(cnt)
        if area < 10:
            continue

        # å»ºç«‹é®ç½©ä¸¦æ‰¾æœ€æ·±é»ž
        mask = np.zeros_like(anomaly_map, dtype=np.uint8)
        cv2.drawContours(mask, [cnt], -1, 1, -1)
        masked_anomaly = np.where(mask == 1, anomaly_map, -np.inf)
        max_idx = np.unravel_index(np.argmax(masked_anomaly), anomaly_map.shape)
        cx, cy = max_idx[1], max_idx[0]
        depth = anomaly_map[max_idx]

        # ðŸ”¸ ç•«ä¸è¦å‰‡é‚Šç•Œ
        cv2.drawContours(annotated, [cnt], -1, (255, 0, 0), 1)

        # ðŸ”¸ æ¨™è¨»æ–‡å­—åœ¨æœ€æ·±é»ž
        label1 = f"#{i+1}"
        label2 = f"a:{int(area)}"
        label3 = f"d:{depth:.2f}"

        line_height = 8 # æ¯è¡Œåž‚ç›´é–“è·ï¼ˆå¯å¾®èª¿ï¼‰
        cv2.putText(annotated, label1, (cx, cy - line_height * 2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.putText(annotated, label2, (cx, cy - line_height),     cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.putText(annotated, label3, (cx, cy),                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.circle(annotated, (cx, cy), 1, (255, 0, 0), -1)  # ç´…è‰²å°åœ“é»ž

    return annotated

import pandas as pd
import os

def save_record_to_csv(record, csv_path="depreciation_records.csv"):
    df = pd.DataFrame([record])  # å–®ç­†è½‰æˆ DataFrame
    if os.path.exists(csv_path):
        df.to_csv(csv_path, mode='a', header=False, index=False)
    else:
        df.to_csv(csv_path, index=False)
        
import re
def safe_load(path, map_location="cpu", weights_only=True, extra_globals=None):
    """
    è‡ªå‹•å®‰å…¨è¼‰å…¥ torch æ¨¡åž‹æª”æ¡ˆ (.pth)
    æœƒæ ¹æ“šéŒ¯èª¤è¨Šæ¯è‡ªå‹•å°‡ç¼ºå°‘çš„ global é¡žåˆ¥/å‡½å¼åŠ å…¥å®‰å…¨æ¸…å–®
    
    Args:
        path (str): æ¨¡åž‹æª”æ¡ˆè·¯å¾‘
        map_location: è¼‰å…¥åˆ°å“ªå€‹è£ç½® (é è¨­ "cpu")
        weights_only (bool): æ˜¯å¦åªè¼‰å…¥æ¬Šé‡ (æŽ¨è–¦ True)
        extra_globals (list): é¡å¤–è¦å…è¨±çš„é¡žåˆ¥ï¼Œä¾‹å¦‚ [MyModel]
    """
    if extra_globals:
        torch.serialization.add_safe_globals(extra_globals)

    while True:
        try:
            return torch.load(path, map_location=map_location, weights_only=weights_only)
        except Exception as e:
            msg = str(e)
            # å˜—è©¦è§£æžéŒ¯èª¤è¨Šæ¯ä¸­çš„é¡žåˆ¥åç¨±
            match = re.search(r"Unsupported global: GLOBAL (.+?) ", msg)
            if match:
                global_name = match.group(1)
                print(f"âš ï¸ æª”æ¡ˆéœ€è¦å…è¨±ï¼š{global_name}")

                # å°æ–¼å…§å»ºé¡žåž‹ (ä¾‹å¦‚ builtins.set)ï¼Œç”¨ eval å–åˆ°å°è±¡
                try:
                    obj = eval(global_name.replace("builtins.", ""))
                    torch.serialization.add_safe_globals([obj])
                    print(f"âœ… å·²å…è¨± {global_name}")
                except Exception as e2:
                    print(f"âŒ ç„¡æ³•è‡ªå‹•å…è¨± {global_name}, è«‹æ‰‹å‹•åŠ å…¥: {e2}")
                    raise
            else:
                raise  # ä¸æ˜¯ Unsupported global å°±ç›´æŽ¥ä¸Ÿå‡º

# ===== ä¸»ç¨‹å¼ =====
if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    # 1.ä¸€è¡Œè¼‰å…¥å®Œæ•´æ¨¡åž‹ï¼ˆéœ€è¨“ç·´æ™‚æ•´å€‹ torch æ¨¡åž‹ä¿å­˜ï¼‰
    model = torch.load("fullmodel_wres50_bottle.pth", map_location=device, weights_only=False)
    model.to(device).eval()
    # feats, recons = model(img_tensor)

    # 2.è®€å–å–®å¼µåœ–ç‰‡
    img_bgr = cv2.imread("test_bottle.png")
    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    img_resized = cv2.resize(img_rgb, (256, 256))
    img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0
    img_tensor = img_tensor.to(device)

    # 3.æŽ¨è«–
    with torch.no_grad():
        feats, recons = model(img_tensor)  # å‡è¨­ä½ çš„æ¨¡åž‹ forward å›žå‚³ (features, reconstructions)
        anomaly_map, _ = cal_anomaly_map([feats[-1]], [recons[-1]], img_tensor.shape[-1])
        anomaly_map = gaussian_filter(anomaly_map, sigma=4)
        ano_map_norm = min_max_norm(anomaly_map) * 255
        ano_map_color = cvt2heatmap(ano_map_norm)

    # 4.ç–ŠåŠ ç†±åŠ›åœ–
    overlay = show_cam_on_image(img_resized, ano_map_color)

    # 5.å„²å­˜
    cv2.imwrite("heatmap_overlay.png", cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
    print("âœ… å–®å¼µå½±åƒç¼ºé™·ç†±åŠ›åœ–å·²å®Œæˆ â†’ heatmap_overlay.png")
    
    # 6. ç¼ºé™·å€åŸŸåˆ†æž/ç¼ºé™·å€åŸŸæ¨™è¨»
    defects = extract_defect_regions(anomaly_map, threshold=0.8)
    for i, d in enumerate(defects):
        print(f"ðŸ”§ ç¼ºé™· {i+1}: é¢ç©={d['area']:.1f}, ä¸­å¿ƒ={d['center']}, é•·å¯¬={d['size']}, æ·±åº¦={d['depth']:.3f}")
    annotated_img = extract_and_annotate_defects(img_resized, anomaly_map, threshold=0.8)
    cv2.imwrite("heatmap_annotated.png", cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR))
    print("ðŸ“Œ ç¼ºé™·å€åŸŸå·²æ¨™è¨» â†’ heatmap_annotated.png")
    
    # 7.æŠ˜èˆŠåˆ†æž
    record = generate_depreciation_record(defects)
    # âœ… å°å‡ºæŠ˜èˆŠåˆ†æžçµæžœ
    print(f"\nðŸ“Š æŠ˜èˆŠåˆ†æžå ±å‘Šï¼ˆ{record['timestamp']}ï¼‰")
    print(f"ç­‰ç´šï¼š{record['grade']}")
    print(f"ç¼ºé™·æ•¸é‡ï¼š{record['defect_count']}")
    print(f"ç¸½é¢ç©ï¼š{record['total_area']:.1f}")
    print(f"å¹³å‡æ·±åº¦ï¼š{record['avg_depth']:.2f}")
    print(f"æœ€å¤§æ·±åº¦ï¼š{record['max_depth']:.2f}")
    print(f"æŠ˜èˆŠæŒ‡æ•¸ï¼š{record['defect_index']:.2f}")
    
    #8.åŠ å…¥ç´€éŒ„è³‡æ–™çš„CSVå„²å­˜
    save_record_to_csv(record)
    print("âœ… å·²å„²å­˜ç´€éŒ„è‡³ CSV")
    #9.è¨“ç·´MLPæ¨¡åž‹
    # å¯é¸ï¼šæ¯æ–°å¢ž 50 ç­†å°± retrain
    if len(pd.read_csv("depreciation_records.csv")) % 1 == 0:
        train_mlp_from_csv()
    print("âœ… å·²é‡æ–°è¨“ç·´ MLP æ¨¡åž‹")
    # âœ… æŠŠ DepreciationMLP åŠ é€² PyTorch å®‰å…¨æ¸…å–®
    mlp_model = safe_load(
        "depreciation_mlp.pth",
        map_location=device,   # æˆ– device
        weights_only=True,
        extra_globals=[DepreciationMLP]  # å…ˆåŠ è‡ªå·±çš„æ¨¡åž‹ class
    )
    # torch.serialization.add_safe_globals([DepreciationMLP,set])
    # è¼‰å…¥ MLP æ¨¡åž‹ï¼ˆéœ€äº‹å…ˆè¨“ç·´å¥½ä¸¦å„²å­˜ï¼‰
    # mlp_model = torch.load("depreciation_mlp.pth", map_location=device,weights_only=True)
    mlp_model.eval()

    # æŠ˜èˆŠåˆ†æžï¼ˆä½¿ç”¨ MLPï¼‰
    record = generate_depreciation_record(defects, mlp_model=mlp_model)
    # âœ… å°å‡ºå®Œæ•´æŠ˜èˆŠåˆ†æžç´€éŒ„ï¼ˆå« MLP ç­‰ç´šï¼‰
print("\nðŸ“Š æŠ˜èˆŠåˆ†æžç´€éŒ„ï¼ˆä½¿ç”¨ MLP æ¨¡åž‹ï¼‰")
for key, value in record.items():
    if key != "defects":
        print(f"{key}: {value}")
    else:
        print(f"{key}:")
        for i, defect in enumerate(value):
            print(f"  ðŸ”§ ç¼ºé™· {i+1}: é¢ç©={defect['area']:.1f}, ä¸­å¿ƒ={defect['center']}, é•·å¯¬={defect['size']}, æ·±åº¦={defect['depth']:.3f}")