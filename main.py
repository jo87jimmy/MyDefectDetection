import torch  # åŒ¯å…¥ PyTorchï¼Œç”¨æ–¼å¼µé‡é‹ç®—èˆ‡æ¨¡å‹æ§‹å»º
import cv2  # åŒ¯å…¥ OpenCVï¼Œç”¨æ–¼å½±åƒè™•ç†
import numpy as np  # åŒ¯å…¥ NumPyï¼Œç”¨æ–¼æ•¸å€¼é‹ç®—
from scipy.ndimage import gaussian_filter  # åŒ¯å…¥é«˜æ–¯æ¿¾æ³¢å™¨ï¼Œç”¨æ–¼å¹³æ»‘ anomaly map
import torch.nn.functional as F  # åŒ¯å…¥ PyTorch çš„å‡½å¼åº«ï¼Œç”¨æ–¼è¨ˆç®— cosine similarity ç­‰
from train_depreciation_mlp import save_record_to_csv, interpret_comparison_results, generate_enhanced_depreciation_record, generate_comparison_depreciation_record_with_excel, EnhancedDepreciationMLP, train_enhanced_mlp_from_csv  # åŒ¯å…¥æ”¹è‰¯ç‰ˆ MLP é¡åˆ¥èˆ‡åˆ†æå‡½å¼
import pickle
import os
import pandas as pd


class FullModel(torch.nn.Module):
    """ æ¨¡çµ„åŒ¯å…¥èˆ‡æ¨¡å‹å®šç¾©ï¼ŒåŒ…å« encoderã€batch normalizationã€decoder """

    def __init__(self, encoder, bn, decoder):
        super().__init__()
        self.encoder = encoder  # ç·¨ç¢¼å™¨ï¼Œç”¨æ–¼æå–ç‰¹å¾µ
        self.bn = bn  # æ‰¹æ¬¡æ­£è¦åŒ–å±¤
        self.decoder = decoder  # è§£ç¢¼å™¨ï¼Œç”¨æ–¼é‡å»ºå½±åƒ

    def forward(self, x):
        feats = self.encoder(x)  # æå–ç‰¹å¾µ
        recons = self.decoder(self.bn(feats))  # ç¶“é BN å¾Œé‡å»ºå½±åƒ
        return feats, recons  # å›å‚³ç‰¹å¾µèˆ‡é‡å»ºçµæœ


def min_max_norm(image):
    """å°‡è¼¸å…¥çš„å½±åƒï¼ˆé€šå¸¸æ˜¯ anomaly map æˆ–ç°éšåœ–ï¼‰é€²è¡Œã€Œæœ€å°æœ€å¤§å€¼æ­£è¦åŒ–ã€ï¼Œ
    è®“æ‰€æœ‰åƒç´ å€¼éƒ½è½åœ¨ 0 åˆ° 1 ä¹‹é–“ã€‚è®“å¾ŒçºŒçš„è™•ç†ï¼ˆä¾‹å¦‚äºŒå€¼åŒ–ã€ç†±åŠ›åœ–è½‰æ›ï¼‰æ›´ç©©å®šä¸€è‡´ã€‚
    image.min()ï¼šæ‰¾å‡ºå½±åƒä¸­æœ€æš—çš„åƒç´ å€¼
    image.max()ï¼šæ‰¾å‡ºå½±åƒä¸­æœ€äº®çš„åƒç´ å€¼
    (image - a_min) / (a_max - a_min + 1e-8)ï¼šå°‡æ‰€æœ‰åƒç´ å€¼ä¾ç…§æœ€å¤§æœ€å°ç¯„åœé€²è¡Œç¸®æ”¾ï¼Œ
    1e-8 æ˜¯ç‚ºäº†é¿å…é™¤ä»¥ 0 çš„éŒ¯èª¤ã€‚"""
    a_min, a_max = image.min(), image.max()  # å–å¾—å½±åƒä¸­æœ€å°å€¼èˆ‡æœ€å¤§å€¼
    return (image - a_min) / (a_max - a_min + 1e-8)  # å°‡å½±åƒé€²è¡Œ Min-Max æ­£è¦åŒ–ï¼Œé¿å…é™¤ä»¥ 0


def cvt2heatmap(gray):
    """å°‡ç°éšå½±åƒï¼ˆé€šå¸¸æ˜¯ anomaly map æˆ–æ­£è¦åŒ–å¾Œçš„æ·±åº¦åœ–ï¼‰è½‰æ›æˆå½©è‰²ç†±åŠ›åœ–ï¼Œè®“ç¼ºé™·å€åŸŸçš„è¦–è¦ºæ•ˆæœæ›´æ˜é¡¯ã€‚
    np.uint8(gray)ï¼šå°‡ç°éšå½±åƒè½‰æˆ 8-bit æ ¼å¼ï¼ˆ0â€“255ï¼‰ï¼Œä»¥ç¬¦åˆ OpenCV çš„è‰²å½©æ˜ å°„éœ€æ±‚
    cv2.COLORMAP_JETï¼šä½¿ç”¨ JET è‰²ç›¤ï¼Œå¸¸è¦‹æ–¼ç†±åŠ›åœ–ï¼Œé¡è‰²å¾è— â†’ ç¶  â†’ é»ƒ â†’ ç´…ï¼Œç´…è‰²ä»£è¡¨é«˜å€¼ï¼ˆé€šå¸¸æ˜¯ç¼ºé™·æœ€æ·±è™•ï¼‰
    é€™å€‹æ­¥é©Ÿé€šå¸¸æœƒæ¥åœ¨ min_max_norm ä¹‹å¾Œï¼Œè®“ anomaly map çš„é«˜å€¼å€åŸŸåœ¨ç†±åŠ›åœ–ä¸­å‘ˆç¾ç´…è‰²ã€ä½å€¼å€åŸŸå‘ˆç¾è—è‰²ï¼Œæ–¹ä¾¿è¾¨è­˜ã€‚ """
    heatmap = cv2.applyColorMap(np.uint8(gray),
                                cv2.COLORMAP_JET)  # å°‡ç°éšå½±åƒè½‰æ›ç‚ºå½©è‰²ç†±åŠ›åœ–ï¼ˆä½¿ç”¨ JET è‰²ç›¤ï¼‰
    return heatmap  # å›å‚³å½©è‰²ç†±åŠ›åœ–


def show_cam_on_image(img, anomaly_map):
    """å°‡anomaly mapï¼ˆé€šå¸¸æ˜¯ç†±åŠ›åœ–ï¼‰ç–ŠåŠ åœ¨åŸå§‹å½±åƒä¸Šï¼Œç”¢ç”Ÿè¦–è¦ºåŒ–çš„ç–Šåœ–æ•ˆæœï¼Œè®“ç¼ºé™·å€åŸŸä¸€ç›®äº†ç„¶ã€‚
    np.float32(...) / 255ï¼šå°‡å½±åƒèˆ‡ anomaly map éƒ½è½‰æˆ 0~1 çš„æµ®é»æ ¼å¼ï¼Œæ–¹ä¾¿é€²è¡ŒåŠ æ³•ç–ŠåŠ 
    cam = cam / np.max(cam)ï¼šå°‡ç–ŠåŠ å¾Œçš„çµæœå†æ­£è¦åŒ–ï¼Œç¢ºä¿æ•´é«”äº®åº¦ä¸è¶…é 1
    np.uint8(255 * cam)ï¼šå°‡çµæœè½‰å› 0~255 çš„æ•´æ•¸æ ¼å¼ï¼Œæ–¹ä¾¿ç”¨ OpenCV å„²å­˜æˆ–é¡¯ç¤º
    é€šå¸¸æœƒåœ¨ cvt2heatmap ä¹‹å¾ŒåŸ·è¡Œï¼Œè®“ anomaly map çš„å½©è‰²ç†±åŠ›åœ–èƒ½å¤ èˆ‡åŸå§‹ RGB å½±åƒèåˆï¼Œç”¢ç”Ÿæ¸…æ¥šçš„è¦–è¦ºæç¤ºã€‚ """
    cam = np.float32(anomaly_map) / 255 + np.float32(
        img) / 255  # å°‡ anomaly map ç–ŠåŠ åœ¨åŸå§‹å½±åƒä¸Šï¼ˆéƒ½è½‰ç‚º 0~1 ç¯„åœï¼‰
    cam = cam / np.max(cam)  # æ­£è¦åŒ–ç–ŠåŠ çµæœï¼Œé¿å…è¶…é 1
    return np.uint8(255 * cam)  # å°‡çµæœè½‰å› 0~255 çš„ uint8 æ ¼å¼ï¼Œæ–¹ä¾¿å„²å­˜æˆ–é¡¯ç¤º


def cal_anomaly_map(fs_list, ft_list, out_size=224, amap_mode='a'):
    """     æ ¹æ“šæ¨¡å‹çš„ç‰¹å¾µè¼¸å‡ºèˆ‡é‡å»ºçµæœï¼Œè¨ˆç®— anomaly mapï¼ˆç•°å¸¸åˆ†ä½ˆåœ–ï¼‰ï¼Œç”¨ä¾†åˆ¤æ–·å½±åƒä¸­å“ªäº›å€åŸŸå¯èƒ½å­˜åœ¨ç¼ºé™·ã€‚
        fs_listï¼šåŸå§‹ç‰¹å¾µåˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯ encoder çš„è¼¸å‡ºï¼‰
        ft_listï¼šé‡å»ºç‰¹å¾µåˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯ decoder çš„è¼¸å‡ºï¼‰
        F.cosine_similarity(fs, ft)ï¼šè¨ˆç®—æ¯å€‹ä½ç½®çš„ç‰¹å¾µç›¸ä¼¼åº¦ï¼Œå€¼è¶Šå°ä»£è¡¨è¶Šç•°å¸¸
        1 - ç›¸ä¼¼åº¦ï¼šå°‡ç›¸ä¼¼åº¦è½‰ç‚ºç•°å¸¸åˆ†æ•¸ï¼ˆé«˜åˆ†ä»£è¡¨ç•°å¸¸ï¼‰
        F.interpolate(...)ï¼šå°‡æ¯å±¤çš„ anomaly map æ”¾å¤§åˆ°çµ±ä¸€å°ºå¯¸ï¼Œæ–¹ä¾¿èåˆèˆ‡è¦–è¦ºåŒ–
        anomaly_map += a_mapï¼šå°‡å¤šå±¤ anomaly map ç–ŠåŠ ï¼Œå½¢æˆæœ€çµ‚çš„ç•°å¸¸åˆ†ä½ˆåœ–
        é€šå¸¸æœƒæ¥åœ¨æ¨¡å‹æ¨è«–ä¹‹å¾Œï¼Œä¸¦åœ¨ gaussian_filter å¹³æ»‘å¾Œé€²è¡Œè¦–è¦ºåŒ–èˆ‡ç¼ºé™·åˆ†æã€‚ """
    anomaly_map = np.zeros([out_size, out_size])  # åˆå§‹åŒ– anomaly mapï¼Œå¤§å°ç‚ºè¼¸å‡ºå°ºå¯¸
    a_map_list = []  # å„²å­˜æ¯å±¤çš„ anomaly mapï¼ˆå¯é¸ï¼‰
    for fs, ft in zip(fs_list, ft_list):  # éæ­·æ¯ä¸€å±¤çš„ç‰¹å¾µèˆ‡é‡å»ºçµæœ
        a_map = 1 - F.cosine_similarity(
            fs, ft)  # è¨ˆç®— cosine ç›¸ä¼¼åº¦ï¼Œè½‰ç‚º anomaly åˆ†æ•¸ï¼ˆè¶Šä¸ç›¸ä¼¼è¶Šç•°å¸¸ï¼‰
        a_map = torch.unsqueeze(a_map,
                                dim=1)  # å¢åŠ  channel ç¶­åº¦ï¼Œç¬¦åˆ interpolate çš„è¼¸å…¥æ ¼å¼
        a_map = F.interpolate(a_map,
                              size=out_size,
                              mode='bilinear',
                              align_corners=True)  # å°‡ anomaly map æ”¾å¤§åˆ°æŒ‡å®šå°ºå¯¸
        a_map = a_map[0,
                      0, :, :].cpu().numpy()  # è½‰ç‚º NumPy æ ¼å¼ä¸¦ç§»é™¤ batch/channel ç¶­åº¦
        a_map_list.append(a_map)  # å„²å­˜æ¯å±¤çš„ anomaly map
        anomaly_map += a_map  # ç´¯åŠ  anomaly mapï¼ˆå¤šå±¤èåˆï¼‰
    return anomaly_map, a_map_list  # å›å‚³æœ€çµ‚ anomaly map èˆ‡æ¯å±¤çš„ map æ¸…å–®


def extract_defect_regions(anomaly_map, threshold=0.6):
    """å¾ anomaly map ä¸­åµæ¸¬ä¸¦æå–ç¼ºé™·å€åŸŸçš„æ ¸å¿ƒé‚è¼¯
    æœƒæ ¹æ“š anomaly map çš„ç•°å¸¸åˆ†ä½ˆï¼Œåµæ¸¬å‡ºæ‰€æœ‰æ˜é¡¯çš„ç¼ºé™·å€åŸŸï¼Œä¸¦é‡å°æ¯å€‹å€åŸŸè¨ˆç®—ï¼š
    é¢ç©ï¼ˆareaï¼‰
    ä¸­å¿ƒé»ï¼ˆæœ€æ·±é»ï¼‰
    å¯¬é«˜ï¼ˆbounding boxï¼‰
    æ·±åº¦ï¼ˆæœ€å¤§ anomaly å€¼ï¼‰
    è¼ªå»“ï¼ˆcontourï¼‰
    å¾ŒçºŒçš„æŠ˜èˆŠåˆ†ææ¨¡çµ„ä½¿ç”¨ï¼Œä¾†è©•ä¼°ç‰©ä»¶çš„å¥åº·ç‹€æ…‹èˆ‡ç¶­ä¿®å»ºè­°ã€‚ """
    norm_map = min_max_norm(anomaly_map)  # å°‡ anomaly map æ­£è¦åŒ–åˆ° 0~1 ç¯„åœ
    binary_mask = (norm_map
                   > threshold).astype(np.uint8)  # æ ¹æ“šé–€æª»å€¼å»ºç«‹äºŒå€¼é®ç½©ï¼ˆ1 è¡¨ç¤ºç•°å¸¸ï¼‰
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL,
                                   cv2.CHAIN_APPROX_SIMPLE)  # æ‰¾å‡ºæ‰€æœ‰å¤–éƒ¨è¼ªå»“ï¼ˆç¼ºé™·å€åŸŸï¼‰
    defects = []  # å„²å­˜æ‰€æœ‰ç¼ºé™·è³‡è¨Š
    for cnt in contours:  # éæ­·æ¯å€‹è¼ªå»“
        area = cv2.contourArea(cnt)  # è¨ˆç®—è¼ªå»“é¢ç©
        if area < 10:  # å¿½ç•¥å¤ªå°çš„å€åŸŸï¼ˆå¯èƒ½æ˜¯é›œè¨Šï¼‰
            continue
        x, y, w, h = cv2.boundingRect(cnt)  # å–å¾—ç¼ºé™·å€åŸŸçš„é‚Šç•Œæ¡†ï¼ˆå·¦ä¸Šè§’åº§æ¨™èˆ‡å¯¬é«˜ï¼‰
        # å»ºç«‹é®ç½©ä¸¦æ‰¾å‡º anomaly map ä¸­è©²å€åŸŸæœ€æ·±çš„é»ï¼ˆæœ€å¤§å€¼ï¼‰
        mask = np.zeros_like(anomaly_map,
                             dtype=np.uint8)  # å»ºç«‹èˆ‡ anomaly map åŒå¤§å°çš„é®ç½©
        cv2.drawContours(mask, [cnt], -1, 1, -1)  # å°‡è¼ªå»“å¡«æ»¿åœ¨é®ç½©ä¸Š
        masked_anomaly = np.where(mask == 1, anomaly_map,
                                  -np.inf)  # åªä¿ç•™é®ç½©å…§çš„ anomaly å€¼ï¼Œå…¶é¤˜è¨­ç‚º -âˆ
        max_idx = np.unravel_index(np.argmax(masked_anomaly),
                                   anomaly_map.shape)  # æ‰¾å‡ºæœ€å¤§å€¼çš„ä½ç½®ï¼ˆæœ€æ·±é»ï¼‰
        cx, cy = max_idx[1], max_idx[0]  # OpenCV åº§æ¨™é †åºç‚º (x, y)ï¼Œæ‰€ä»¥è¦åè½‰
        depth = anomaly_map[max_idx]  # å–å¾—æœ€æ·±é»çš„ anomaly å€¼ï¼ˆä»£è¡¨ç¼ºé™·æ·±åº¦ï¼‰
        defects.append({
            "area": float(area),  # ç¢ºä¿æ˜¯ Python float
            "center": (int(cx), int(cy)),  # ç¢ºä¿æ˜¯ Python int
            "size": (int(w), int(h)),  # ç¢ºä¿æ˜¯ Python int
            "depth": float(depth),  # ç¢ºä¿æ˜¯ Python float
            "contour": cnt  # å°‡è¼ªå»“ä¸€ä½µå›å‚³
        })
    return defects  # å›å‚³æ‰€æœ‰ç¼ºé™·è³‡è¨Šåˆ—è¡¨


def extract_and_annotate_defects(img, anomaly_map, threshold=0.6):
    """
    ç”¨ä¾†å°‡ç¼ºé™·å€åŸŸæ¨™è¨»åœ¨åŸå§‹å½±åƒä¸Šï¼Œä¸¦åŠ ä¸Šæ–‡å­—èªªæ˜èˆ‡è¦–è¦ºæç¤º
    æ ¹æ“š anomaly map çš„ç•°å¸¸åˆ†ä½ˆï¼Œå°‡åµæ¸¬åˆ°çš„ç¼ºé™·å€åŸŸï¼š
    ç”¨è—è‰²è¼ªå»“æ¡†å‡º
    åœ¨æœ€æ·±é»åŠ ä¸Šç´…è‰²æ–‡å­—æ¨™è¨»ï¼ˆç·¨è™Ÿã€é¢ç©ã€æ·±åº¦ï¼‰
    ç•«å‡ºç´…è‰²å°åœ“é»ä½œç‚ºè¦–è¦ºç„¦é»
    é©åˆç”¨æ–¼å ±è¡¨ã€GUIã€dashboard é¡¯ç¤ºï¼Œä»¥æŒæ¡ç¼ºé™·ä½ç½®èˆ‡åš´é‡ç¨‹åº¦ã€‚
    """
    # ç‚ºäº†è®“è¦–è¦ºåŒ–è¼ªå»“æ›´å¹³æ»‘ï¼Œå° anomaly map é€²è¡Œé«˜æ–¯æ¨¡ç³Šã€‚
    # é€™ä¸€æ­¥é©Ÿä¸å½±éŸ¿æ ¸å¿ƒæ•¸æ“šï¼ˆå¦‚æ·±åº¦ï¼‰çš„æº–ç¢ºæ€§ã€‚
    smoothed_map = cv2.GaussianBlur(anomaly_map, (5, 5), sigmaX=2)

    # ç›´æ¥å‘¼å«æ ¸å¿ƒå‡½å¼ä¾†æå–æ‰€æœ‰ç¼ºé™·çš„è©³ç´°è³‡è¨Š
    # æˆ‘å€‘å‚³å…¥å¹³æ»‘å¾Œçš„ mapï¼Œä»¥ç²å¾—èˆ‡è¦–è¦ºä¸Šä¸€è‡´çš„è¼ªå»“
    defects = extract_defect_regions(smoothed_map, threshold)

    annotated = img.copy()  # å»ºç«‹å½±åƒå‰¯æœ¬ï¼Œç”¨ä¾†ç¹ªè£½æ¨™è¨»

    # éæ­·æ‰€æœ‰åµæ¸¬åˆ°çš„ç¼ºé™·ï¼Œä¸¦åœ¨å½±åƒä¸Šé€²è¡Œæ¨™è¨»
    for i, defect in enumerate(defects):
        # å¾ defect ç‰©ä»¶ä¸­ç›´æ¥å–å¾—æ‰€éœ€è³‡è¨Š
        cnt = defect["contour"]
        area = defect["area"]
        cx, cy = defect["center"]

        # ç‚ºäº†æ•¸æ“šçš„ç²¾æº–åº¦ï¼Œæ·±åº¦å€¼æ‡‰å¾ã€ŒåŸå§‹ã€çš„ anomaly_map ä¸­ç²å–ï¼Œ
        # å› ç‚º smoothed_map çš„æ•¸å€¼å¯èƒ½å› æ¨¡ç³Šè€Œç•¥å¾®é™ä½ã€‚
        depth = anomaly_map[cy, cx]

        # ğŸ”¸ ç•«å‡ºä¸è¦å‰‡é‚Šç•Œï¼ˆè—è‰²ï¼‰
        cv2.drawContours(annotated, [cnt], -1, (255, 0, 0), 1)

        # ğŸ”¸ åœ¨æœ€æ·±é»æ¨™è¨»æ–‡å­—ï¼ˆç´…è‰²ï¼‰
        label1 = f"#{i+1}"  # ç¼ºé™·ç·¨è™Ÿ
        label2 = f"a:{int(area)}"  # é¢ç©
        label3 = f"d:{depth:.2f}"  # æ·±åº¦ (ä½¿ç”¨åŸå§‹æ·±åº¦)
        line_height = 8  # æ¯è¡Œæ–‡å­—çš„å‚ç›´é–“è·

        cv2.putText(annotated, label1, (cx, cy - line_height * 2),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.putText(annotated, label2, (cx, cy - line_height),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.putText(annotated, label3, (cx, cy), cv2.FONT_HERSHEY_SIMPLEX, 0.3,
                    (0, 0, 255), 1)

        # ğŸ”¸ åœ¨æœ€æ·±é»ç•«ç´…è‰²å°åœ“é»ä½œç‚ºè¦–è¦ºç„¦é»
        cv2.circle(annotated, (cx, cy), 1, (255, 0, 0), -1)

    return annotated  # å›å‚³å·²æ¨™è¨»çš„å½±åƒ


def clean_csv_file(csv_path):
    """æ¸…ç†æå£çš„ CSV æª”æ¡ˆï¼Œç§»é™¤æ ¼å¼éŒ¯èª¤çš„è¡Œ"""
    try:
        # é€è¡Œè®€å–ä¸¦é©—è­‰
        valid_lines = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        if not lines:
            return

        # ä¿ç•™æ¨™é¡Œè¡Œ
        header = lines[0].strip()
        valid_lines.append(header)
        expected_fields = len(header.split(','))

        # æª¢æŸ¥æ¯ä¸€è¡Œ
        for i, line in enumerate(lines[1:], 1):
            line = line.strip()
            if not line:
                continue

            # ç°¡å–®çš„æ¬„ä½æ•¸é‡æª¢æŸ¥
            fields = line.split(',')
            if len(fields) == expected_fields:
                valid_lines.append(line)
            else:
                print(
                    f"âš ï¸ è·³éç¬¬ {i+1} è¡Œï¼ˆæ¬„ä½æ•¸ä¸ç¬¦ï¼‰: {len(fields)} vs {expected_fields}"
                )

        # é‡å¯«æª”æ¡ˆ
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(valid_lines))

        print(f"âœ… å·²æ¸…ç† CSV æª”æ¡ˆï¼Œä¿ç•™ {len(valid_lines)-1} ç­†æœ‰æ•ˆè¨˜éŒ„")

    except Exception as e:
        print(f"âŒ æ¸…ç† CSV æª”æ¡ˆå¤±æ•—: {e}")


import argparse
# ===== ä¸»ç¨‹å¼ =====
if __name__ == "__main__":  # åˆ¤æ–·æ˜¯å¦ç‚ºä¸»ç¨‹å¼åŸ·è¡Œï¼ˆé¿å…è¢«å…¶ä»–æ¨¡çµ„åŒ¯å…¥æ™‚åŸ·è¡Œï¼‰
    parser = argparse.ArgumentParser()
    parser.add_argument('--category', default='bottle', type=str)  # è¨“ç·´é¡åˆ¥
    args = parser.parse_args()
    device = "cuda" if torch.cuda.is_available(
    ) else "cpu"  # æ ¹æ“šç’°å¢ƒé¸æ“‡ GPU æˆ– CPU è£ç½®
    # 1. è¼‰å…¥å®Œæ•´æ¨¡å‹ï¼ˆéœ€ä½¿ç”¨ torch.save(model) å„²å­˜çš„æ¨¡å‹ï¼‰
    model = torch.load("fullmodel_wres50_bottle.pth",
                       map_location=device,
                       weights_only=False)
    model.to(device).eval()  # ç§»è‡³æŒ‡å®šè£ç½®ä¸¦è¨­ç‚ºæ¨è«–æ¨¡å¼
    test_path = './mvtec/' + args.category + '/test'  # æ¸¬è©¦è³‡æ–™è·¯å¾‘
    items = ['good', 'broken_large', 'broken_small', 'contamination']  # æ¸¬è©¦è³‡æ–™æ¨™ç±¤
    print(f"ğŸ” æ¸¬è©¦è³‡æ–™å¤¾ï¼š{test_path}ï¼Œå…± {len(items)} é¡åˆ¥")

# ä¾é¡åˆ¥é€å¼µè®€å–å½±åƒä¸¦åŸ·è¡Œæ¨è«–
for item in items:
    item_path = os.path.join(test_path, item)
    img_files = [
        f for f in os.listdir(item_path)
        if f.endswith('.png') or f.endswith('.jpg')
    ]

    print(f"\nğŸ“‚ é¡åˆ¥ï¼š{item}ï¼Œå…± {len(img_files)} å¼µå½±åƒ")

    for img_name in img_files:
        img_path = os.path.join(item_path, img_name)
        print(f"\nğŸ–¼ï¸ è™•ç†å½±åƒï¼š{img_path}")

        # åŸæœ‰çš„å½±åƒé è™•ç†å’Œæ¨¡å‹æ¨è«–ä¿æŒä¸è®Š
        img_bgr = cv2.imread(img_path)
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (256, 256))
        img_tensor = torch.from_numpy(img_resized).permute(
            2, 0, 1).float().unsqueeze(0) / 255.0
        img_tensor = img_tensor.to(device)

        # æ¨¡å‹æ¨è«–
        with torch.no_grad():
            feats, recons = model(img_tensor)
            anomaly_map, _ = cal_anomaly_map([feats[-1]], [recons[-1]],
                                             img_tensor.shape[-1])
            anomaly_map = gaussian_filter(anomaly_map, sigma=4)

        # åŸæœ‰çš„è¦–è¦ºåŒ–è¼¸å‡ºä¿æŒä¸è®Š
        ano_map_norm = min_max_norm(anomaly_map) * 255
        ano_map_color = cvt2heatmap(ano_map_norm)
        overlay = show_cam_on_image(img_resized, ano_map_color)
        overlay_path = f"results/{item}_{img_name}_overlay.png"
        cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))

        # ç¼ºé™·æå–
        defects = extract_defect_regions(anomaly_map, threshold=0.8)
        for i, d in enumerate(defects):
            print(
                f"ğŸ”§ ç¼ºé™· {i+1}: é¢ç©={d['area']:.1f}, ä¸­å¿ƒ={d['center']}, é•·å¯¬={d['size']}, æ·±åº¦={d['depth']:.3f}"
            )

        annotated_img = extract_and_annotate_defects(img_resized,
                                                     anomaly_map,
                                                     threshold=0.8)
        annotated_path = f"results/{item}_{img_name}_annotated.png"
        cv2.imwrite(annotated_path,
                    cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR))
        print(f"ğŸ“Œ ç¼ºé™·æ¨™è¨»å·²å„²å­˜ â†’ {annotated_path}")

        # === æ”¹è‰¯ç‰ˆæŠ˜èˆŠåˆ†æé–‹å§‹ ===
        # è¼‰å…¥æ”¹è‰¯ç‰ˆçµ„ä»¶
        enhanced_mlp_model = None
        scaler = None

        # å˜—è©¦è¼‰å…¥æ¨™æº–åŒ–å™¨
        if os.path.exists("feature_scaler.pkl"):
            with open("feature_scaler.pkl", 'rb') as f:
                scaler = pickle.load(f)

        # å˜—è©¦è¼‰å…¥æ”¹è‰¯ç‰ˆæ¨¡å‹
        if os.path.exists("enhanced_depreciation_mlp.pth") and scaler:
            try:
                # æª¢æŸ¥ scaler çš„ç‰¹å¾µæ•¸é‡ä¾†æ±ºå®šæ¨¡å‹è¼¸å…¥ç¶­åº¦
                scaler_features = scaler.n_features_in_
                enhanced_mlp_model = EnhancedDepreciationMLP(
                    input_dim=scaler_features)
                enhanced_mlp_model.load_state_dict(
                    torch.load("enhanced_depreciation_mlp.pth",
                               weights_only=True))
                enhanced_mlp_model.eval()
                print(f"ğŸ“‚ å·²è¼‰å…¥æ”¹è‰¯ç‰ˆ MLP æ¨¡å‹ (ç‰¹å¾µæ•¸: {scaler_features})")
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ”¹è‰¯ç‰ˆæ¨¡å‹å¤±æ•—: {e}")
                enhanced_mlp_model = None

        # ä½¿ç”¨æ”¹è‰¯ç‰ˆåˆ†æ
        record = generate_enhanced_depreciation_record(defects,
                                                       enhanced_mlp_model,
                                                       scaler,
                                                       image_shape=(256, 256))
        # #å°ç…§çµ„
        recordB = generate_comparison_depreciation_record_with_excel(
            defects, enhanced_mlp_model, scaler, image_shape=(256, 256))
        interpret_comparison_results(recordB)
        # save_record_to_csv(
        #     record, csv_path="generate_comparison_depreciation_record.csv")
        # === æ”¹è‰¯ç‰ˆæŠ˜èˆŠåˆ†æçµæŸ ===

        # å„²å­˜è¨˜éŒ„
        save_record_to_csv(record)

        # æ”¹è‰¯ç‰ˆé‡è¨“ç·´æ¢ä»¶
        try:
            # å˜—è©¦è®€å– CSVï¼Œå¦‚æœå¤±æ•—å‰‡æ¸…ç†å¾Œé‡è©¦
            df = pd.read_csv("depreciation_records.csv")
        except pd.errors.ParserError as e:
            print(f"âš ï¸ CSV æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
            print("ğŸ”§ å˜—è©¦ä¿®å¾© CSV æª”æ¡ˆ...")

            # å‚™ä»½åŸæª”æ¡ˆ
            import shutil
            shutil.copy("depreciation_records.csv",
                        "depreciation_records_backup.csv")

            # é‡æ–°å»ºç«‹ä¹¾æ·¨çš„ CSV
            clean_csv_file("depreciation_records.csv")

            # é‡æ–°è®€å–
            try:
                df = pd.read_csv("depreciation_records.csv")
                print("âœ… CSV æª”æ¡ˆå·²ä¿®å¾©")
            except Exception as e2:
                print(f"âŒ ç„¡æ³•ä¿®å¾© CSV æª”æ¡ˆ: {e2}")
                # å»ºç«‹ç©ºçš„ DataFrame ç¹¼çºŒåŸ·è¡Œ
                df = pd.DataFrame()
        # åœ¨è®€å– CSV æˆåŠŸå¾ŒåŠ å…¥é‡è¨“ç·´é‚è¼¯
        if not df.empty:
            # æ›´æ™ºèƒ½çš„é‡è¨“ç·´æ¢ä»¶ï¼šè‡³å°‘50ç­†æ•¸æ“šï¼Œæ¯20ç­†é‡è¨“ç·´ä¸€æ¬¡
            if len(df) >= 50 and len(df) % 20 == 0:
                print("ğŸ”„ è§¸ç™¼æ”¹è‰¯ç‰ˆ MLP é‡è¨“ç·´...")
                try:
                    #todo
                    enhanced_mlp_model, scaler = train_enhanced_mlp_from_csv()
                    print("âœ… æ”¹è‰¯ç‰ˆ MLP é‡è¨“ç·´å®Œæˆ")
                except Exception as e:
                    print(f"âš ï¸ é‡è¨“ç·´å¤±æ•—: {e}")
