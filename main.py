import torch  # åŒ¯å…¥ PyTorchï¼Œç”¨æ–¼å¼µé‡é‹ç®—èˆ‡æ¨¡å‹æ§‹å»º
import cv2  # åŒ¯å…¥ OpenCVï¼Œç”¨æ–¼å½±åƒè™•ç†
import numpy as np  # åŒ¯å…¥ NumPyï¼Œç”¨æ–¼æ•¸å€¼é‹ç®—
from scipy.ndimage import gaussian_filter  # åŒ¯å…¥é«˜æ–¯æ¿¾æ³¢å™¨ï¼Œç”¨æ–¼å¹³æ»‘ anomaly map
import torch.nn.functional as F  # åŒ¯å…¥ PyTorch çš„å‡½å¼åº«ï¼Œç”¨æ–¼è¨ˆç®— cosine similarity ç­‰
from train_depreciation_mlp import DepreciationMLP,train_mlp_from_csv,generate_depreciation_record  # å¾è‡ªè¨‚æ¨¡çµ„åŒ¯å…¥ MLP é¡åˆ¥èˆ‡è¨“ç·´ã€åˆ†æå‡½å¼
from train_depreciation_mlp import generate_enhanced_depreciation_record, EnhancedDepreciationMLP,train_enhanced_mlp_from_csv  # åŒ¯å…¥æ”¹è‰¯ç‰ˆ MLP é¡åˆ¥èˆ‡åˆ†æå‡½å¼
import pickle
from sklearn.preprocessing import StandardScaler
class FullModel(torch.nn.Module):
    """ æ¨¡çµ„åŒ¯å…¥èˆ‡æ¨¡å‹å®šç¾©ï¼ŒåŒ…å« encoderã€batch normalizationã€decoder """
    def __init__(self, encoder, bn, decoder):
        super().__init__()
        self.encoder = encoder  # ç·¨ç¢¼å™¨ï¼Œç”¨æ–¼æå–ç‰¹å¾µ
        self.bn = bn  # æ‰¹æ¬¡æ­£è¦åŒ–å±¤
        self.decoder = decoder  # è§£ç¢¼å™¨ï¼Œç”¨æ–¼é‡å»ºå½±åƒ

    def forward(self, x):
        feats = self.encoder(x)  # æå–ç‰¹å¾µ
        recons = self.decoder(self.bn(feats))  # ç¶“é BN å¾Œé‡å»ºå½±åƒ
        return feats, recons  # å›å‚³ç‰¹å¾µèˆ‡é‡å»ºçµæœ

def min_max_norm(image):
    """å°‡è¼¸å…¥çš„å½±åƒï¼ˆé€šå¸¸æ˜¯ anomaly map æˆ–ç°éšåœ–ï¼‰é€²è¡Œã€Œæœ€å°æœ€å¤§å€¼æ­£è¦åŒ–ã€ï¼Œ
    è®“æ‰€æœ‰åƒç´ å€¼éƒ½è½åœ¨ 0 åˆ° 1 ä¹‹é–“ã€‚è®“å¾ŒçºŒçš„è™•ç†ï¼ˆä¾‹å¦‚äºŒå€¼åŒ–ã€ç†±åŠ›åœ–è½‰æ›ï¼‰æ›´ç©©å®šä¸€è‡´ã€‚
    image.min()ï¼šæ‰¾å‡ºå½±åƒä¸­æœ€æš—çš„åƒç´ å€¼
    image.max()ï¼šæ‰¾å‡ºå½±åƒä¸­æœ€äº®çš„åƒç´ å€¼
    (image - a_min) / (a_max - a_min + 1e-8)ï¼šå°‡æ‰€æœ‰åƒç´ å€¼ä¾ç…§æœ€å¤§æœ€å°ç¯„åœé€²è¡Œç¸®æ”¾ï¼Œ
    1e-8 æ˜¯ç‚ºäº†é¿å…é™¤ä»¥ 0 çš„éŒ¯èª¤ã€‚"""
    a_min, a_max = image.min(), image.max()  # å–å¾—å½±åƒä¸­æœ€å°å€¼èˆ‡æœ€å¤§å€¼
    return (image - a_min) / (a_max - a_min + 1e-8)  # å°‡å½±åƒé€²è¡Œ Min-Max æ­£è¦åŒ–ï¼Œé¿å…é™¤ä»¥ 0

def cvt2heatmap(gray):
    """å°‡ç°éšå½±åƒï¼ˆé€šå¸¸æ˜¯ anomaly map æˆ–æ­£è¦åŒ–å¾Œçš„æ·±åº¦åœ–ï¼‰è½‰æ›æˆå½©è‰²ç†±åŠ›åœ–ï¼Œè®“ç¼ºé™·å€åŸŸçš„è¦–è¦ºæ•ˆæœæ›´æ˜é¡¯ã€‚
    np.uint8(gray)ï¼šå°‡ç°éšå½±åƒè½‰æˆ 8-bit æ ¼å¼ï¼ˆ0â€“255ï¼‰ï¼Œä»¥ç¬¦åˆ OpenCV çš„è‰²å½©æ˜ å°„éœ€æ±‚
    cv2.COLORMAP_JETï¼šä½¿ç”¨ JET è‰²ç›¤ï¼Œå¸¸è¦‹æ–¼ç†±åŠ›åœ–ï¼Œé¡è‰²å¾è— â†’ ç¶  â†’ é»ƒ â†’ ç´…ï¼Œç´…è‰²ä»£è¡¨é«˜å€¼ï¼ˆé€šå¸¸æ˜¯ç¼ºé™·æœ€æ·±è™•ï¼‰
    é€™å€‹æ­¥é©Ÿé€šå¸¸æœƒæ¥åœ¨ min_max_norm ä¹‹å¾Œï¼Œè®“ anomaly map çš„é«˜å€¼å€åŸŸåœ¨ç†±åŠ›åœ–ä¸­å‘ˆç¾ç´…è‰²ã€ä½å€¼å€åŸŸå‘ˆç¾è—è‰²ï¼Œæ–¹ä¾¿è¾¨è­˜ã€‚ """
    heatmap = cv2.applyColorMap(np.uint8(gray), cv2.COLORMAP_JET)  # å°‡ç°éšå½±åƒè½‰æ›ç‚ºå½©è‰²ç†±åŠ›åœ–ï¼ˆä½¿ç”¨ JET è‰²ç›¤ï¼‰
    return heatmap  # å›å‚³å½©è‰²ç†±åŠ›åœ–

def show_cam_on_image(img, anomaly_map):
    """å°‡anomaly mapï¼ˆé€šå¸¸æ˜¯ç†±åŠ›åœ–ï¼‰ç–ŠåŠ åœ¨åŸå§‹å½±åƒä¸Šï¼Œç”¢ç”Ÿè¦–è¦ºåŒ–çš„ç–Šåœ–æ•ˆæœï¼Œè®“ç¼ºé™·å€åŸŸä¸€ç›®äº†ç„¶ã€‚
    np.float32(...) / 255ï¼šå°‡å½±åƒèˆ‡ anomaly map éƒ½è½‰æˆ 0~1 çš„æµ®é»æ ¼å¼ï¼Œæ–¹ä¾¿é€²è¡ŒåŠ æ³•ç–ŠåŠ 
    cam = cam / np.max(cam)ï¼šå°‡ç–ŠåŠ å¾Œçš„çµæœå†æ­£è¦åŒ–ï¼Œç¢ºä¿æ•´é«”äº®åº¦ä¸è¶…é 1
    np.uint8(255 * cam)ï¼šå°‡çµæœè½‰å› 0~255 çš„æ•´æ•¸æ ¼å¼ï¼Œæ–¹ä¾¿ç”¨ OpenCV å„²å­˜æˆ–é¡¯ç¤º
    é€šå¸¸æœƒåœ¨ cvt2heatmap ä¹‹å¾ŒåŸ·è¡Œï¼Œè®“ anomaly map çš„å½©è‰²ç†±åŠ›åœ–èƒ½å¤ èˆ‡åŸå§‹ RGB å½±åƒèåˆï¼Œç”¢ç”Ÿæ¸…æ¥šçš„è¦–è¦ºæç¤ºã€‚ """
    cam = np.float32(anomaly_map) / 255 + np.float32(img) / 255  # å°‡ anomaly map ç–ŠåŠ åœ¨åŸå§‹å½±åƒä¸Šï¼ˆéƒ½è½‰ç‚º 0~1 ç¯„åœï¼‰
    cam = cam / np.max(cam)  # æ­£è¦åŒ–ç–ŠåŠ çµæœï¼Œé¿å…è¶…é 1
    return np.uint8(255 * cam)  # å°‡çµæœè½‰å› 0~255 çš„ uint8 æ ¼å¼ï¼Œæ–¹ä¾¿å„²å­˜æˆ–é¡¯ç¤º

def cal_anomaly_map(fs_list, ft_list, out_size=224, amap_mode='a'):
    """     æ ¹æ“šæ¨¡å‹çš„ç‰¹å¾µè¼¸å‡ºèˆ‡é‡å»ºçµæœï¼Œè¨ˆç®— anomaly mapï¼ˆç•°å¸¸åˆ†ä½ˆåœ–ï¼‰ï¼Œç”¨ä¾†åˆ¤æ–·å½±åƒä¸­å“ªäº›å€åŸŸå¯èƒ½å­˜åœ¨ç¼ºé™·ã€‚
        fs_listï¼šåŸå§‹ç‰¹å¾µåˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯ encoder çš„è¼¸å‡ºï¼‰
        ft_listï¼šé‡å»ºç‰¹å¾µåˆ—è¡¨ï¼ˆé€šå¸¸æ˜¯ decoder çš„è¼¸å‡ºï¼‰
        F.cosine_similarity(fs, ft)ï¼šè¨ˆç®—æ¯å€‹ä½ç½®çš„ç‰¹å¾µç›¸ä¼¼åº¦ï¼Œå€¼è¶Šå°ä»£è¡¨è¶Šç•°å¸¸
        1 - ç›¸ä¼¼åº¦ï¼šå°‡ç›¸ä¼¼åº¦è½‰ç‚ºç•°å¸¸åˆ†æ•¸ï¼ˆé«˜åˆ†ä»£è¡¨ç•°å¸¸ï¼‰
        F.interpolate(...)ï¼šå°‡æ¯å±¤çš„ anomaly map æ”¾å¤§åˆ°çµ±ä¸€å°ºå¯¸ï¼Œæ–¹ä¾¿èåˆèˆ‡è¦–è¦ºåŒ–
        anomaly_map += a_mapï¼šå°‡å¤šå±¤ anomaly map ç–ŠåŠ ï¼Œå½¢æˆæœ€çµ‚çš„ç•°å¸¸åˆ†ä½ˆåœ–
        é€šå¸¸æœƒæ¥åœ¨æ¨¡å‹æ¨è«–ä¹‹å¾Œï¼Œä¸¦åœ¨ gaussian_filter å¹³æ»‘å¾Œé€²è¡Œè¦–è¦ºåŒ–èˆ‡ç¼ºé™·åˆ†æã€‚ """
    anomaly_map = np.zeros([out_size, out_size])  # åˆå§‹åŒ– anomaly mapï¼Œå¤§å°ç‚ºè¼¸å‡ºå°ºå¯¸
    a_map_list = []  # å„²å­˜æ¯å±¤çš„ anomaly mapï¼ˆå¯é¸ï¼‰
    for fs, ft in zip(fs_list, ft_list):  # éæ­·æ¯ä¸€å±¤çš„ç‰¹å¾µèˆ‡é‡å»ºçµæœ
        a_map = 1 - F.cosine_similarity(fs, ft)  # è¨ˆç®— cosine ç›¸ä¼¼åº¦ï¼Œè½‰ç‚º anomaly åˆ†æ•¸ï¼ˆè¶Šä¸ç›¸ä¼¼è¶Šç•°å¸¸ï¼‰
        a_map = torch.unsqueeze(a_map, dim=1)  # å¢åŠ  channel ç¶­åº¦ï¼Œç¬¦åˆ interpolate çš„è¼¸å…¥æ ¼å¼
        a_map = F.interpolate(a_map, size=out_size, mode='bilinear', align_corners=True)  # å°‡ anomaly map æ”¾å¤§åˆ°æŒ‡å®šå°ºå¯¸
        a_map = a_map[0, 0, :, :].cpu().numpy()  # è½‰ç‚º NumPy æ ¼å¼ä¸¦ç§»é™¤ batch/channel ç¶­åº¦
        a_map_list.append(a_map)  # å„²å­˜æ¯å±¤çš„ anomaly map
        anomaly_map += a_map  # ç´¯åŠ  anomaly mapï¼ˆå¤šå±¤èåˆï¼‰
    return anomaly_map, a_map_list  # å›å‚³æœ€çµ‚ anomaly map èˆ‡æ¯å±¤çš„ map æ¸…å–®

def extract_defect_regions(anomaly_map, threshold=0.6):
    """å¾ anomaly map ä¸­åµæ¸¬ä¸¦æå–ç¼ºé™·å€åŸŸçš„æ ¸å¿ƒé‚è¼¯
    æœƒæ ¹æ“š anomaly map çš„ç•°å¸¸åˆ†ä½ˆï¼Œåµæ¸¬å‡ºæ‰€æœ‰æ˜é¡¯çš„ç¼ºé™·å€åŸŸï¼Œä¸¦é‡å°æ¯å€‹å€åŸŸè¨ˆç®—ï¼š
    é¢ç©ï¼ˆareaï¼‰
    ä¸­å¿ƒé»ï¼ˆæœ€æ·±é»ï¼‰
    å¯¬é«˜ï¼ˆbounding boxï¼‰
    æ·±åº¦ï¼ˆæœ€å¤§ anomaly å€¼ï¼‰
    å¾ŒçºŒçš„æŠ˜èˆŠåˆ†ææ¨¡çµ„ä½¿ç”¨ï¼Œä¾†è©•ä¼°ç‰©ä»¶çš„å¥åº·ç‹€æ…‹èˆ‡ç¶­ä¿®å»ºè­°ã€‚ """
    norm_map = min_max_norm(anomaly_map)  # å°‡ anomaly map æ­£è¦åŒ–åˆ° 0~1 ç¯„åœ
    binary_mask = (norm_map > threshold).astype(np.uint8)  # æ ¹æ“šé–€æª»å€¼å»ºç«‹äºŒå€¼é®ç½©ï¼ˆ1 è¡¨ç¤ºç•°å¸¸ï¼‰
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # æ‰¾å‡ºæ‰€æœ‰å¤–éƒ¨è¼ªå»“ï¼ˆç¼ºé™·å€åŸŸï¼‰
    defects = []  # å„²å­˜æ‰€æœ‰ç¼ºé™·è³‡è¨Š
    for cnt in contours:  # éæ­·æ¯å€‹è¼ªå»“
        area = cv2.contourArea(cnt)  # è¨ˆç®—è¼ªå»“é¢ç©
        if area < 10:  # å¿½ç•¥å¤ªå°çš„å€åŸŸï¼ˆå¯èƒ½æ˜¯é›œè¨Šï¼‰
            continue
        x, y, w, h = cv2.boundingRect(cnt)  # å–å¾—ç¼ºé™·å€åŸŸçš„é‚Šç•Œæ¡†ï¼ˆå·¦ä¸Šè§’åº§æ¨™èˆ‡å¯¬é«˜ï¼‰
        # å»ºç«‹é®ç½©ä¸¦æ‰¾å‡º anomaly map ä¸­è©²å€åŸŸæœ€æ·±çš„é»ï¼ˆæœ€å¤§å€¼ï¼‰
        mask = np.zeros_like(anomaly_map, dtype=np.uint8)  # å»ºç«‹èˆ‡ anomaly map åŒå¤§å°çš„é®ç½©
        cv2.drawContours(mask, [cnt], -1, 1, -1)  # å°‡è¼ªå»“å¡«æ»¿åœ¨é®ç½©ä¸Š
        masked_anomaly = np.where(mask == 1, anomaly_map, -np.inf)  # åªä¿ç•™é®ç½©å…§çš„ anomaly å€¼ï¼Œå…¶é¤˜è¨­ç‚º -âˆ
        max_idx = np.unravel_index(np.argmax(masked_anomaly), anomaly_map.shape)  # æ‰¾å‡ºæœ€å¤§å€¼çš„ä½ç½®ï¼ˆæœ€æ·±é»ï¼‰
        cx, cy = max_idx[1], max_idx[0]  # OpenCV åº§æ¨™é †åºç‚º (x, y)ï¼Œæ‰€ä»¥è¦åè½‰
        depth = anomaly_map[max_idx]  # å–å¾—æœ€æ·±é»çš„ anomaly å€¼ï¼ˆä»£è¡¨ç¼ºé™·æ·±åº¦ï¼‰
        defects.append({
        "area": float(area),  # ç¢ºä¿æ˜¯ Python float
        "center": (int(cx), int(cy)),  # ç¢ºä¿æ˜¯ Python int
        "size": (int(w), int(h)),  # ç¢ºä¿æ˜¯ Python int
        "depth": float(depth)  # ç¢ºä¿æ˜¯ Python float
        })
    return defects  # å›å‚³æ‰€æœ‰ç¼ºé™·è³‡è¨Šåˆ—è¡¨

def extract_and_annotate_defects(img, anomaly_map, threshold=0.6):
    """
    ç”¨ä¾†å°‡ç¼ºé™·å€åŸŸæ¨™è¨»åœ¨åŸå§‹å½±åƒä¸Šï¼Œä¸¦åŠ ä¸Šæ–‡å­—èªªæ˜èˆ‡è¦–è¦ºæç¤º
    æ ¹æ“š anomaly map çš„ç•°å¸¸åˆ†ä½ˆï¼Œå°‡åµæ¸¬åˆ°çš„ç¼ºé™·å€åŸŸï¼š
    ç”¨è—è‰²è¼ªå»“æ¡†å‡º
    åœ¨æœ€æ·±é»åŠ ä¸Šç´…è‰²æ–‡å­—æ¨™è¨»ï¼ˆç·¨è™Ÿã€é¢ç©ã€æ·±åº¦ï¼‰
    ç•«å‡ºç´…è‰²å°åœ“é»ä½œç‚ºè¦–è¦ºç„¦é»
    é©åˆç”¨æ–¼å ±è¡¨ã€GUIã€dashboard é¡¯ç¤ºï¼Œä»¥æŒæ¡ç¼ºé™·ä½ç½®èˆ‡åš´é‡ç¨‹åº¦ã€‚
    """
    smoothed_map = cv2.GaussianBlur(anomaly_map, (5, 5), sigmaX=2)  # å° anomaly map å¥—ç”¨é«˜æ–¯æ¨¡ç³Šï¼Œå¹³æ»‘é‚Šç·£
    norm_map = min_max_norm(smoothed_map)  # å°‡æ¨¡ç³Šå¾Œçš„ anomaly map æ­£è¦åŒ–åˆ° 0~1
    binary_mask = (norm_map > threshold).astype(np.uint8) * 255  # æ ¹æ“šé–€æª»å€¼å»ºç«‹äºŒå€¼é®ç½©ï¼ˆ255 è¡¨ç¤ºç•°å¸¸å€åŸŸï¼‰
    contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)  # æ‰¾å‡ºæ‰€æœ‰å¤–éƒ¨è¼ªå»“ï¼ˆç¼ºé™·å€åŸŸï¼‰
    annotated = img.copy()  # å»ºç«‹å½±åƒå‰¯æœ¬ï¼Œç”¨ä¾†ç¹ªè£½æ¨™è¨»
    for i, cnt in enumerate(contours):  # éæ­·æ¯å€‹è¼ªå»“
        area = cv2.contourArea(cnt)  # è¨ˆç®—è¼ªå»“é¢ç©
        if area < 10:  # å¿½ç•¥å¤ªå°çš„å€åŸŸï¼ˆå¯èƒ½æ˜¯é›œè¨Šï¼‰
            continue
        # å»ºç«‹é®ç½©ä¸¦æ‰¾å‡º anomaly map ä¸­è©²å€åŸŸæœ€æ·±çš„é»ï¼ˆæœ€å¤§å€¼ï¼‰
        mask = np.zeros_like(anomaly_map, dtype=np.uint8)  # å»ºç«‹èˆ‡ anomaly map åŒå¤§å°çš„é®ç½©
        cv2.drawContours(mask, [cnt], -1, 1, -1)  # å°‡è¼ªå»“å¡«æ»¿åœ¨é®ç½©ä¸Š
        masked_anomaly = np.where(mask == 1, anomaly_map, -np.inf)  # åªä¿ç•™é®ç½©å…§çš„ anomaly å€¼ï¼Œå…¶é¤˜è¨­ç‚º -âˆ
        max_idx = np.unravel_index(np.argmax(masked_anomaly), anomaly_map.shape)  # æ‰¾å‡ºæœ€å¤§å€¼çš„ä½ç½®ï¼ˆæœ€æ·±é»ï¼‰
        cx, cy = max_idx[1], max_idx[0]  # OpenCV åº§æ¨™é †åºç‚º (x, y)ï¼Œæ‰€ä»¥è¦åè½‰
        depth = anomaly_map[max_idx]  # å–å¾—æœ€æ·±é»çš„ anomaly å€¼ï¼ˆä»£è¡¨ç¼ºé™·æ·±åº¦ï¼‰
        # ğŸ”¸ ç•«å‡ºä¸è¦å‰‡é‚Šç•Œï¼ˆè—è‰²ï¼‰
        cv2.drawContours(annotated, [cnt], -1, (255, 0, 0), 1)
        # ğŸ”¸ åœ¨æœ€æ·±é»æ¨™è¨»æ–‡å­—ï¼ˆç´…è‰²ï¼‰
        label1 = f"#{i+1}"  # ç¼ºé™·ç·¨è™Ÿ
        label2 = f"a:{int(area)}"  # é¢ç©
        label3 = f"d:{depth:.2f}"  # æ·±åº¦
        line_height = 8  # æ¯è¡Œæ–‡å­—çš„å‚ç›´é–“è·ï¼ˆå¯å¾®èª¿ï¼‰
        cv2.putText(annotated, label1, (cx, cy - line_height * 2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.putText(annotated, label2, (cx, cy - line_height),     cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.putText(annotated, label3, (cx, cy),                   cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 1)
        cv2.circle(annotated, (cx, cy), 1, (255, 0, 0), -1)  # åœ¨æœ€æ·±é»ç•«ç´…è‰²å°åœ“é»
    return annotated  # å›å‚³å·²æ¨™è¨»çš„å½±åƒ

import pandas as pd
import os
import json
import numpy as np
def save_record_to_csv(record, csv_path="depreciation_records.csv"):
    """æ”¹è‰¯ç‰ˆ CSV å„²å­˜å‡½æ•¸ï¼Œç¢ºä¿æ•¸æ“šæ ¼å¼æ­£ç¢ºä¸¦è™•ç† NumPy é¡å‹"""
    record_copy = record.copy()

    if 'defects' in record_copy:
        # è½‰æ› NumPy é¡å‹ç‚º Python åŸç”Ÿé¡å‹
        def convert_numpy_types(obj):
            if isinstance(obj, dict):
                return {k: convert_numpy_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, (np.integer, np.int64, np.int32)):
                return int(obj)
            elif isinstance(obj, (np.floating, np.float64, np.float32)):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj

        # æ¸…ç† defects æ•¸æ“šä¸¦è½‰ç‚º JSON
        cleaned_defects = convert_numpy_types(record_copy['defects'])
        record_copy['defects'] = json.dumps(cleaned_defects)

    # åŒæ™‚æ¸…ç†å…¶ä»–å¯èƒ½çš„ NumPy é¡å‹
    for key, value in record_copy.items():
        if isinstance(value, (np.integer, np.int64, np.int32)):
            record_copy[key] = int(value)
        elif isinstance(value, (np.floating, np.float64, np.float32)):
            record_copy[key] = float(value)

    df = pd.DataFrame([record_copy])

    try:
        if os.path.exists(csv_path):
            df.to_csv(csv_path, mode='a', header=False, index=False)
        else:
            df.to_csv(csv_path, index=False)
        print(f"âœ… è¨˜éŒ„å·²å„²å­˜è‡³ {csv_path}")
    except Exception as e:
        print(f"âš ï¸ å„²å­˜ CSV å¤±æ•—: {e}")

import re
def safe_load(path, map_location="cpu", weights_only=True, extra_globals=None):
    """
    è‡ªå‹•å®‰å…¨è¼‰å…¥ torch æ¨¡å‹æª”æ¡ˆ (.pth)
    æœƒæ ¹æ“šéŒ¯èª¤è¨Šæ¯è‡ªå‹•å°‡ç¼ºå°‘çš„ global é¡åˆ¥/å‡½å¼åŠ å…¥å®‰å…¨æ¸…å–®
    Args:
        path (str): æ¨¡å‹æª”æ¡ˆè·¯å¾‘
        map_location: è¼‰å…¥åˆ°å“ªå€‹è£ç½® (é è¨­ "cpu")
        weights_only (bool): æ˜¯å¦åªè¼‰å…¥æ¬Šé‡ (æ¨è–¦ True)
        extra_globals (list): é¡å¤–è¦å…è¨±çš„é¡åˆ¥ï¼Œä¾‹å¦‚ [MyModel]
    """
    if extra_globals:
        torch.serialization.add_safe_globals(extra_globals)# è‹¥æœ‰æä¾›é¡å¤–é¡åˆ¥ï¼Œå…ˆåŠ å…¥å®‰å…¨æ¸…å–®
    while True:
        try:
            return torch.load(path, map_location=map_location, weights_only=weights_only)# å˜—è©¦è¼‰å…¥æ¨¡å‹
        except Exception as e:
            msg = str(e)# å–å¾—éŒ¯èª¤è¨Šæ¯æ–‡å­—
            # å˜—è©¦è§£æéŒ¯èª¤è¨Šæ¯ä¸­çš„é¡åˆ¥åç¨±ï¼ˆé€šå¸¸æ˜¯ç¼ºå°‘çš„ global é¡åˆ¥ï¼‰
            match = re.search(r"Unsupported global: GLOBAL (.+?) ", msg)
            if match:
                global_name = match.group(1)# å–å¾—ç¼ºå°‘çš„é¡åˆ¥åç¨±
                print(f"âš ï¸ æª”æ¡ˆéœ€è¦å…è¨±ï¼š{global_name}")
                # å°æ–¼å…§å»ºé¡å‹ (ä¾‹å¦‚ builtins.set)ï¼Œç”¨ eval å–åˆ°å°è±¡
                try:
                    obj = eval(global_name.replace("builtins.", ""))# å°‡æ–‡å­—è½‰ç‚ºç‰©ä»¶
                    torch.serialization.add_safe_globals([obj])# åŠ å…¥å®‰å…¨æ¸…å–®
                    print(f"âœ… å·²å…è¨± {global_name}")
                except Exception as e2:
                    print(f"âŒ ç„¡æ³•è‡ªå‹•å…è¨± {global_name}, è«‹æ‰‹å‹•åŠ å…¥: {e2}")# è‹¥ç„¡æ³•è‡ªå‹•è™•ç†å‰‡æç¤ºæ‰‹å‹•åŠ å…¥
                    raise # ä¸Ÿå‡ºä¾‹å¤–ï¼Œçµ‚æ­¢è¼‰å…¥æµç¨‹
            else:
                raise  # ä¸æ˜¯ Unsupported global å°±ç›´æ¥ä¸Ÿå‡º

def enhanced_defect_analysis_pipeline():
    """æ”¹è‰¯ç‰ˆç¼ºé™·åˆ†ææµç¨‹"""

    # è¼‰å…¥æ¨™æº–åŒ–å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    scaler = None
    scaler_path = "feature_scaler.pkl"
    if os.path.exists(scaler_path):
        with open(scaler_path, 'rb') as f:
            scaler = pickle.load(f)

    # è¼‰å…¥æ”¹è‰¯ç‰ˆ MLP æ¨¡å‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    enhanced_mlp_model = None
    enhanced_model_path = "enhanced_depreciation_mlp.pth"
    if os.path.exists(enhanced_model_path) and scaler:
        enhanced_mlp_model = EnhancedDepreciationMLP()
        enhanced_mlp_model.load_state_dict(torch.load(enhanced_model_path, weights_only=True))
        enhanced_mlp_model.eval()

    return enhanced_mlp_model, scaler

# åœ¨ä¸»è¦è™•ç†è¿´åœˆä¸­ä½¿ç”¨æ”¹è‰¯ç‰ˆåŠŸèƒ½
def process_image_with_enhancements(img_tensor, model, device):
    """ä½¿ç”¨æ”¹è‰¯ç‰ˆåŠŸèƒ½è™•ç†å–®å¼µå½±åƒ"""

    # è¼‰å…¥æ”¹è‰¯ç‰ˆçµ„ä»¶
    enhanced_mlp_model, scaler = enhanced_defect_analysis_pipeline()

    # åŸæœ‰çš„ç•°å¸¸æª¢æ¸¬æµç¨‹
    with torch.no_grad():
        feats, recons = model(img_tensor)
        anomaly_map, _ = cal_anomaly_map([feats[-1]], [recons[-1]], img_tensor.shape[-1])
        anomaly_map = gaussian_filter(anomaly_map, sigma=4)

    # ç¼ºé™·æå–
    defects = extract_defect_regions(anomaly_map, threshold=0.8)

    # ä½¿ç”¨æ”¹è‰¯ç‰ˆæŠ˜èˆŠåˆ†æ
    if enhanced_mlp_model and scaler:
        record = generate_enhanced_depreciation_record(
            defects, enhanced_mlp_model, scaler, image_shape=(256, 256)
        )
        print(f"ğŸ“Š æ”¹è‰¯ç‰ˆ MLP åˆ†æ - ç­‰ç´š: {record['grade']}, "
              f"ä¿¡å¿ƒ: {record['confidence']:.3f}, ä¸ç¢ºå®šæ€§: {record['uncertainty']:.3f}")
    else:
        # å›é€€åˆ°åŸå§‹æ–¹æ³•
        record = generate_depreciation_record(defects)
        print(f"ğŸ“Š è¦å‰‡å¼åˆ†æ - ç­‰ç´š: {record['grade']}")

    # å„²å­˜è¨˜éŒ„
    try:
        save_record_to_csv(record)
    except Exception as e:
        print(f"âš ï¸ å„²å­˜è¨˜éŒ„å¤±æ•—: {e}")
        # å˜—è©¦ç°¡åŒ–è¨˜éŒ„å†å„²å­˜
        simplified_record = {
            "timestamp": record.get("timestamp", ""),
            "grade": record.get("grade", ""),
            "confidence": record.get("confidence", "N/A"),
            "defect_index": float(record.get("defect_index", 0)),
            "defect_count": int(record.get("defect_count", 0)),
            "avg_depth": float(record.get("avg_depth", 0)),
            "max_depth": float(record.get("max_depth", 0)),
            "total_area": float(record.get("total_area", 0))
        }
        save_record_to_csv(simplified_record)

    # æ¢ä»¶å¼é‡è¨“ç·´ï¼ˆæ”¹è‰¯ç‰ˆï¼‰
    try:
        df = pd.read_csv("depreciation_records.csv")
        # æ›´æ™ºèƒ½çš„é‡è¨“ç·´æ¢ä»¶ï¼šè‡³å°‘50ç­†æ•¸æ“šï¼Œæ¯20ç­†é‡è¨“ç·´ä¸€æ¬¡
        if len(df) >= 50 and len(df) % 20 == 0:
            print("ğŸ”„ è§¸ç™¼æ”¹è‰¯ç‰ˆ MLP é‡è¨“ç·´...")
            enhanced_mlp_model, scaler = train_enhanced_mlp_from_csv()
    except (FileNotFoundError, pd.errors.ParserError) as e:
        print(f"âš ï¸ è®€å–æ­·å²è¨˜éŒ„å¤±æ•—: {e}")
        if isinstance(e, pd.errors.ParserError):
            print("ğŸ”§ å˜—è©¦ä¿®å¾© CSV æª”æ¡ˆ...")
            clean_csv_file("depreciation_records.csv")


    return record, defects
def clean_csv_file(csv_path):
    """æ¸…ç†æå£çš„ CSV æª”æ¡ˆï¼Œç§»é™¤æ ¼å¼éŒ¯èª¤çš„è¡Œ"""
    try:
        # é€è¡Œè®€å–ä¸¦é©—è­‰
        valid_lines = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        if not lines:
            return

        # ä¿ç•™æ¨™é¡Œè¡Œ
        header = lines[0].strip()
        valid_lines.append(header)
        expected_fields = len(header.split(','))

        # æª¢æŸ¥æ¯ä¸€è¡Œ
        for i, line in enumerate(lines[1:], 1):
            line = line.strip()
            if not line:
                continue

            # ç°¡å–®çš„æ¬„ä½æ•¸é‡æª¢æŸ¥
            fields = line.split(',')
            if len(fields) == expected_fields:
                valid_lines.append(line)
            else:
                print(f"âš ï¸ è·³éç¬¬ {i+1} è¡Œï¼ˆæ¬„ä½æ•¸ä¸ç¬¦ï¼‰: {len(fields)} vs {expected_fields}")

        # é‡å¯«æª”æ¡ˆ
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(valid_lines))

        print(f"âœ… å·²æ¸…ç† CSV æª”æ¡ˆï¼Œä¿ç•™ {len(valid_lines)-1} ç­†æœ‰æ•ˆè¨˜éŒ„")

    except Exception as e:
        print(f"âŒ æ¸…ç† CSV æª”æ¡ˆå¤±æ•—: {e}")
import argparse
# ===== ä¸»ç¨‹å¼ =====
if __name__ == "__main__":  # åˆ¤æ–·æ˜¯å¦ç‚ºä¸»ç¨‹å¼åŸ·è¡Œï¼ˆé¿å…è¢«å…¶ä»–æ¨¡çµ„åŒ¯å…¥æ™‚åŸ·è¡Œï¼‰
    parser = argparse.ArgumentParser()
    parser.add_argument('--category', default='bottle', type=str)  # è¨“ç·´é¡åˆ¥
    args = parser.parse_args()
    device = "cuda" if torch.cuda.is_available() else "cpu"  # æ ¹æ“šç’°å¢ƒé¸æ“‡ GPU æˆ– CPU è£ç½®
    # 1. è¼‰å…¥å®Œæ•´æ¨¡å‹ï¼ˆéœ€ä½¿ç”¨ torch.save(model) å„²å­˜çš„æ¨¡å‹ï¼‰
    model = torch.load("fullmodel_wres50_bottle.pth", map_location=device, weights_only=False)
    model.to(device).eval()  # ç§»è‡³æŒ‡å®šè£ç½®ä¸¦è¨­ç‚ºæ¨è«–æ¨¡å¼
    test_path = './mvtec/' + args.category + '/test' # æ¸¬è©¦è³‡æ–™è·¯å¾‘
    items = ['good', 'broken_large', 'broken_small', 'contamination'] # æ¸¬è©¦è³‡æ–™æ¨™ç±¤
    print(f"ğŸ” æ¸¬è©¦è³‡æ–™å¤¾ï¼š{test_path}ï¼Œå…± {len(items)} é¡åˆ¥")

# ä¾é¡åˆ¥é€å¼µè®€å–å½±åƒä¸¦åŸ·è¡Œæ¨è«–
for item in items:
    item_path = os.path.join(test_path, item)
    img_files = [f for f in os.listdir(item_path) if f.endswith('.png') or f.endswith('.jpg')]

    print(f"\nğŸ“‚ é¡åˆ¥ï¼š{item}ï¼Œå…± {len(img_files)} å¼µå½±åƒ")

    for img_name in img_files:
        img_path = os.path.join(item_path, img_name)
        print(f"\nğŸ–¼ï¸ è™•ç†å½±åƒï¼š{img_path}")

        # åŸæœ‰çš„å½±åƒé è™•ç†å’Œæ¨¡å‹æ¨è«–ä¿æŒä¸è®Š
        img_bgr = cv2.imread(img_path)
        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
        img_resized = cv2.resize(img_rgb, (256, 256))
        img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0
        img_tensor = img_tensor.to(device)

        # æ¨¡å‹æ¨è«–
        with torch.no_grad():
            feats, recons = model(img_tensor)
            anomaly_map, _ = cal_anomaly_map([feats[-1]], [recons[-1]], img_tensor.shape[-1])
            anomaly_map = gaussian_filter(anomaly_map, sigma=4)

        # åŸæœ‰çš„è¦–è¦ºåŒ–è¼¸å‡ºä¿æŒä¸è®Š
        ano_map_norm = min_max_norm(anomaly_map) * 255
        ano_map_color = cvt2heatmap(ano_map_norm)
        overlay = show_cam_on_image(img_resized, ano_map_color)
        overlay_path = f"results/{item}_{img_name}_overlay.png"
        cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))

        # ç¼ºé™·æå–
        defects = extract_defect_regions(anomaly_map, threshold=0.8)
        # === æ”¹è‰¯ç‰ˆæŠ˜èˆŠåˆ†æé–‹å§‹ ===
        # è¼‰å…¥æ”¹è‰¯ç‰ˆçµ„ä»¶
        enhanced_mlp_model = None
        scaler = None

        # å˜—è©¦è¼‰å…¥æ¨™æº–åŒ–å™¨
        if os.path.exists("feature_scaler.pkl"):
            with open("feature_scaler.pkl", 'rb') as f:
                scaler = pickle.load(f)

        # å˜—è©¦è¼‰å…¥æ”¹è‰¯ç‰ˆæ¨¡å‹
        if os.path.exists("enhanced_depreciation_mlp.pth") and scaler:
            try:
                # æª¢æŸ¥ scaler çš„ç‰¹å¾µæ•¸é‡ä¾†æ±ºå®šæ¨¡å‹è¼¸å…¥ç¶­åº¦
                scaler_features = scaler.n_features_in_
                enhanced_mlp_model = EnhancedDepreciationMLP(input_dim=scaler_features)
                enhanced_mlp_model.load_state_dict(torch.load("enhanced_depreciation_mlp.pth", weights_only=True))
                enhanced_mlp_model.eval()
                print(f"ğŸ“‚ å·²è¼‰å…¥æ”¹è‰¯ç‰ˆ MLP æ¨¡å‹ (ç‰¹å¾µæ•¸: {scaler_features})")
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ”¹è‰¯ç‰ˆæ¨¡å‹å¤±æ•—: {e}")
                enhanced_mlp_model = None

        # ä½¿ç”¨æ”¹è‰¯ç‰ˆåˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰todoå¯ä»¥æ‹¿æ‰else
        if enhanced_mlp_model and scaler:
            record = generate_enhanced_depreciation_record(
                defects, enhanced_mlp_model, scaler, image_shape=(256, 256)
            )
            print(f"ğŸ“Š æ”¹è‰¯ç‰ˆ MLP åˆ†æ - ç­‰ç´š: {record['grade']}, "
                f"ä¿¡å¿ƒ: {record['confidence']:.3f}, ä¸ç¢ºå®šæ€§: {record['uncertainty']:.3f}")
        else:
            # å›é€€åˆ°åŸå§‹æ–¹æ³•
            record = generate_depreciation_record(defects)
            print(f"ğŸ“Š è¦å‰‡å¼åˆ†æ - ç­‰ç´š: {record['grade']}")

        # å„²å­˜è¨˜éŒ„
        save_record_to_csv(record)

        # æ”¹è‰¯ç‰ˆé‡è¨“ç·´æ¢ä»¶
        try:
            # å˜—è©¦è®€å– CSVï¼Œå¦‚æœå¤±æ•—å‰‡æ¸…ç†å¾Œé‡è©¦
            df = pd.read_csv("depreciation_records.csv")
        except pd.errors.ParserError as e:
            print(f"âš ï¸ CSV æª”æ¡ˆæ ¼å¼éŒ¯èª¤: {e}")
            print("ğŸ”§ å˜—è©¦ä¿®å¾© CSV æª”æ¡ˆ...")

            # å‚™ä»½åŸæª”æ¡ˆ
            import shutil
            shutil.copy("depreciation_records.csv", "depreciation_records_backup.csv")

            # é‡æ–°å»ºç«‹ä¹¾æ·¨çš„ CSV
            clean_csv_file("depreciation_records.csv")

            # é‡æ–°è®€å–
            try:
                df = pd.read_csv("depreciation_records.csv")
                print("âœ… CSV æª”æ¡ˆå·²ä¿®å¾©")
            except Exception as e2:
                print(f"âŒ ç„¡æ³•ä¿®å¾© CSV æª”æ¡ˆ: {e2}")
                # å»ºç«‹ç©ºçš„ DataFrame ç¹¼çºŒåŸ·è¡Œ
                df = pd.DataFrame()

    # for img_name in img_files:
    #     img_path = os.path.join(item_path, img_name)
    #     print(f"\nğŸ–¼ï¸ è™•ç†å½±åƒï¼š{img_path}")

    #     # è®€å–èˆ‡é è™•ç†å½±åƒ
    #     img_bgr = cv2.imread(img_path)
    #     img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)
    #     img_resized = cv2.resize(img_rgb, (256, 256))
    #     img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0
    #     img_tensor = img_tensor.to(device)

    #     # æ¨¡å‹æ¨è«–
    #     with torch.no_grad():
    #         feats, recons = model(img_tensor)
    #         anomaly_map, _ = cal_anomaly_map([feats[-1]], [recons[-1]], img_tensor.shape[-1])
    #         anomaly_map = gaussian_filter(anomaly_map, sigma=4)
    #         ano_map_norm = min_max_norm(anomaly_map) * 255
    #         ano_map_color = cvt2heatmap(ano_map_norm)

    #     # ç–ŠåŠ ç†±åŠ›åœ–
    #     overlay = show_cam_on_image(img_resized, ano_map_color)
    #     overlay_path = f"results/{item}_{img_name}_overlay.png"
    #     cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
    #     print(f"âœ… ç†±åŠ›åœ–å·²å„²å­˜ â†’ {overlay_path}")

    #     # ç¼ºé™·åˆ†æèˆ‡æ¨™è¨»
    #     defects = extract_defect_regions(anomaly_map, threshold=0.8)
    #     for i, d in enumerate(defects):
    #         print(f"ğŸ”§ ç¼ºé™· {i+1}: é¢ç©={d['area']:.1f}, ä¸­å¿ƒ={d['center']}, é•·å¯¬={d['size']}, æ·±åº¦={d['depth']:.3f}")

    #     annotated_img = extract_and_annotate_defects(img_resized, anomaly_map, threshold=0.8)
    #     annotated_path = f"results/{item}_{img_name}_annotated.png"
    #     cv2.imwrite(annotated_path, cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR))
    #     print(f"ğŸ“Œ ç¼ºé™·æ¨™è¨»å·²å„²å­˜ â†’ {annotated_path}")

    #     # æŠ˜èˆŠåˆ†æèˆ‡ç´€éŒ„
    #     record = generate_depreciation_record(defects)
    #     save_record_to_csv(record)

        # # MLP æ¨¡å‹åˆ†æï¼ˆå¯ä¾ç´€éŒ„æ•¸é‡æ¢ä»¶è§¸ç™¼ï¼‰ï¼Œretrain æ¢ä»¶å¯ä¾éœ€æ±‚èª¿æ•´
        # # å¦‚æœæœ‰æ¨¡å‹depreciation_mlp.pth å‰‡è¼‰å…¥ç¹¼çºŒè¨“ç·´ï¼Œæ²’æœ‰å‰‡æ–°å»ºæ¨¡å‹
        # if len(pd.read_csv("depreciation_records.csv")) % 1 == 0:
        #     train_mlp_from_csv()

        # # ä½¿ç”¨ MLP æ¨¡å‹åˆ†æ
        # # 10. è¼‰å…¥ MLP æ¨¡å‹ï¼ˆä½¿ç”¨ state_dict è¼‰å…¥æ¬Šé‡ï¼‰
        # mlp_model = DepreciationMLP()  # å»ºç«‹æ¨¡å‹æ¶æ§‹
        # mlp_model.load_state_dict(torch.load("depreciation_mlp.pth", map_location=device ,weights_only=True))  # è¼‰å…¥æ¬Šé‡
        # mlp_model.eval()  # è¨­ç‚ºæ¨è«–æ¨¡å¼å¼

        # record = generate_depreciation_record(defects, mlp_model=mlp_model)
        # save_record_to_csv(record)
        # print("ğŸ“Š å·²å®Œæˆ MLP æŠ˜èˆŠåˆ†æä¸¦å„²å­˜ç´€éŒ„")




    # # 2. è®€å–å–®å¼µæ¸¬è©¦å½±åƒ
    # img_bgr = cv2.imread("test_bottle.png")  # ä»¥ BGR æ ¼å¼è®€å…¥å½±åƒ
    # img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)  # è½‰æ›ç‚º RGB æ ¼å¼
    # img_resized = cv2.resize(img_rgb, (256, 256))  # èª¿æ•´å°ºå¯¸ç‚º 256x256
    # img_tensor = torch.from_numpy(img_resized).permute(2, 0, 1).float().unsqueeze(0) / 255.0  # è½‰ç‚º PyTorch å¼µé‡ä¸¦æ­£è¦åŒ–
    # img_tensor = img_tensor.to(device)  # ç§»è‡³æŒ‡å®šè£ç½®
    # # 3. æ¨¡å‹æ¨è«–ï¼šå–å¾—ç‰¹å¾µèˆ‡é‡å»ºçµæœ
    # with torch.no_grad():  # åœç”¨æ¢¯åº¦è¨ˆç®—ï¼ˆåŠ é€Ÿæ¨è«–ï¼‰
    #     feats, recons = model(img_tensor)  # åŸ·è¡Œ forwardï¼Œå–å¾—ç‰¹å¾µèˆ‡é‡å»ºå½±åƒ
    #     anomaly_map, _ = cal_anomaly_map([feats[-1]], [recons[-1]], img_tensor.shape[-1])  # è¨ˆç®— anomaly map
    #     anomaly_map = gaussian_filter(anomaly_map, sigma=4)  # å¥—ç”¨é«˜æ–¯æ¨¡ç³Šå¹³æ»‘çµæœ
    #     ano_map_norm = min_max_norm(anomaly_map) * 255  # æ­£è¦åŒ–ä¸¦è½‰ç‚º 0~255 ç¯„åœ
    #     ano_map_color = cvt2heatmap(ano_map_norm)  # è½‰ç‚ºå½©è‰²ç†±åŠ›åœ–
    # # 4. ç–ŠåŠ ç†±åŠ›åœ–è‡³åŸå§‹å½±åƒ
    # overlay = show_cam_on_image(img_resized, ano_map_color)
    # # 5. å„²å­˜ç–Šåœ–çµæœ
    # cv2.imwrite("heatmap_overlay.png", cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))
    # print("âœ… å–®å¼µå½±åƒç¼ºé™·ç†±åŠ›åœ–å·²å®Œæˆ â†’ heatmap_overlay.png")
    # # 6. ç¼ºé™·å€åŸŸåˆ†æèˆ‡æ¨™è¨»
    # defects = extract_defect_regions(anomaly_map, threshold=0.8)  # åµæ¸¬ç¼ºé™·å€åŸŸ
    # for i, d in enumerate(defects):  # å°å‡ºæ¯å€‹ç¼ºé™·çš„è³‡è¨Š
    #     print(f"ğŸ”§ ç¼ºé™· {i+1}: é¢ç©={d['area']:.1f}, ä¸­å¿ƒ={d['center']}, é•·å¯¬={d['size']}, æ·±åº¦={d['depth']:.3f}")
    # annotated_img = extract_and_annotate_defects(img_resized, anomaly_map, threshold=0.8)  # æ¨™è¨»ç¼ºé™·å€åŸŸ
    # cv2.imwrite("heatmap_annotated.png", cv2.cvtColor(annotated_img, cv2.COLOR_RGB2BGR))
    # print("ğŸ“Œ ç¼ºé™·å€åŸŸå·²æ¨™è¨» â†’ heatmap_annotated.png")
    # # 7. æŠ˜èˆŠåˆ†æï¼ˆä½¿ç”¨ rule-based åˆ†ç´šï¼‰
    # record = generate_depreciation_record(defects)
    # print(f"\nğŸ“Š æŠ˜èˆŠåˆ†æå ±å‘Šï¼ˆ{record['timestamp']}ï¼‰")
    # print(f"ç­‰ç´šï¼š{record['grade']}")
    # print(f"ç¼ºé™·æ•¸é‡ï¼š{record['defect_count']}")
    # print(f"ç¸½é¢ç©ï¼š{record['total_area']:.1f}")
    # print(f"å¹³å‡æ·±åº¦ï¼š{record['avg_depth']:.2f}")
    # print(f"æœ€å¤§æ·±åº¦ï¼š{record['max_depth']:.2f}")
    # print(f"æŠ˜èˆŠæŒ‡æ•¸ï¼š{record['defect_index']:.2f}")
    # # 8. å„²å­˜ç´€éŒ„è‡³ CSV æª”æ¡ˆ
    # save_record_to_csv(record)
    # print("âœ… å·²å„²å­˜ç´€éŒ„è‡³ CSV")
    # # 9. è¨“ç·´ MLP æ¨¡å‹ï¼ˆå¯ä¾ç´€éŒ„æ•¸é‡æ¢ä»¶è§¸ç™¼ï¼‰
    # if len(pd.read_csv("depreciation_records.csv")) % 1 == 0:  # æ¯æ–°å¢ 1 ç­†å°± retrainï¼ˆå¯èª¿æ•´æ¢ä»¶ï¼‰
    #     train_mlp_from_csv()
    # print("âœ… å·²é‡æ–°è¨“ç·´ MLP æ¨¡å‹")
    # # 10. è¼‰å…¥ MLP æ¨¡å‹ï¼ˆä½¿ç”¨ safe_load ç¢ºä¿å®‰å…¨ï¼‰
    # mlp_model = safe_load(
    #     "depreciation_mlp.pth",
    #     map_location=device,
    #     weights_only=True,
    #     extra_globals=[DepreciationMLP]  # åŠ å…¥è‡ªè¨‚é¡åˆ¥è‡³å®‰å…¨æ¸…å–®
    # )
    # mlp_model.eval()  # è¨­ç‚ºæ¨è«–æ¨¡å¼
    # # 11. ä½¿ç”¨ MLP æ¨¡å‹é€²è¡ŒæŠ˜èˆŠåˆ†æ
    # record = generate_depreciation_record(defects, mlp_model=mlp_model)
    # print("\nğŸ“Š æŠ˜èˆŠåˆ†æç´€éŒ„ï¼ˆä½¿ç”¨ MLP æ¨¡å‹ï¼‰")

    # for key, value in record.items():
    #     if key == "defects":
    #         print(f"{key}:")  # å°å‡ºç¼ºé™·æ¸…å–®
    #         for i, defect in enumerate(value):
    #             print(f"  ğŸ”§ ç¼ºé™· {i+1}: é¢ç©={defect['area']:.1f}, ä¸­å¿ƒ={defect['center']}, é•·å¯¬={defect['size']}, æ·±åº¦={defect['depth']:.3f}")
    #     elif key == "confidence":
    #         print(f"{key}: {value:.2f}")  # å°å‡ºä¿¡å¿ƒåˆ†æ•¸ï¼ˆä¿ç•™å…©ä½å°æ•¸ï¼‰
    #     else:
    #         print(f"{key}: {value}")  # å°å‡ºå…¶ä»–åˆ†ææŒ‡æ¨™

    # # 12. å„²å­˜ç´€éŒ„è‡³ CSV æª”æ¡ˆ
    # save_record_to_csv(record)
    # print("âœ… å·²å„²å­˜ç´€éŒ„è‡³ CSV")
    # # 13. è¨“ç·´ MLP æ¨¡å‹ï¼ˆå¯ä¾ç´€éŒ„æ•¸é‡æ¢ä»¶è§¸ç™¼ï¼‰
    # if len(pd.read_csv("depreciation_records.csv")) % 1 == 0:  # æ¯æ–°å¢ 1 ç­†å°± retrainï¼ˆå¯èª¿æ•´æ¢ä»¶ï¼‰
    #     train_mlp_from_csv()
    # print("âœ… å·²å®Œæˆ MLP æŠ˜èˆŠåˆ†æ")